# ================================================================
# GenCode Studio - Backend Environment Configuration
# ================================================================
#
# ⚠️ SECURITY WARNING:
# If this .env file was ever committed to version control with real API keys,
# you MUST rotate those keys immediately in the provider consoles:
# - Gemini: https://aistudio.google.com/apikey
# - OpenAI: https://platform.openai.com/api-keys
# - Anthropic: https://console.anthropic.com/settings/keys
#
# Copy this file to .env and fill in your real values.
# ================================================================

# --- Database Configuration ---
# Beanie ODM uses this MongoDB URI for all deployment/provider/project collections
MONGODB_URL=mongodb://localhost:27017/gencode_db

# --- Service Configuration ---
# Absolute path to the workspaces directory where projects are created
# Use forward slashes for cross-platform compatibility
WORKSPACES_PATH=C:/Users/YOUR_USERNAME/path/to/GenCode Studio Python/workspaces

# Docker registry for deployment containers
DOCKER_REGISTRY=localhost:5000

# Base URL exposed by the deployment service (where previews/containers will be accessible)
DEPLOYMENT_BASE_URL=http://localhost:8000

# Internal agents API endpoint (used by workspace routes when calling generate-autonomous)
AGENTS_API_URL=http://localhost:8005

# --- Multi-Provider LLM Configuration ---
# Default provider and model used if none specified per-agent
DEFAULT_LLM_PROVIDER=gemini
DEFAULT_LLM_MODEL=gemini-2.0-flash

# Enable automatic provider fallback on rate limits/errors
ENABLE_PROVIDER_FALLBACK=true

# --- Gemini API Keys ---
# Get your key from: https://aistudio.google.com/apikey
GEMINI_API_KEY=YOUR_GEMINI_API_KEY_HERE

# --- OpenAI Configuration ---
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=YOUR_OPENAI_API_KEY_HERE

# --- Anthropic Configuration ---
# Get your key from: https://console.anthropic.com/settings/keys
ANTHROPIC_API_KEY=YOUR_ANTHROPIC_API_KEY_HERE

# --- Ollama Configuration ---
# Local Ollama instance base URL
OLLAMA_BASE_URL=http://localhost:11434

# --- Integration Manager (Optional) ---
# Set to "false" to disable centralized provider fallback/rotation
INTEGRATION_MANAGER_ENABLED=true

# --- WebSocket Configuration ---
# Port for real-time project updates (if different from main API)
WEBSOCKET_PORT=8000

# --- Development & Debugging ---
# Set to "true" to enable verbose logging of LLM calls and agent activity
DEBUG_MODE=false

# Log file path (optional)
LOG_FILE_PATH=./logs/gencode.log

# --- Security & Secrets ---
# Encryption key for deployment secrets (auto-generated if not provided)
# Generate a secure key for production: openssl rand -hex 32
SECRET_ENCRYPTION_KEY=YOUR_SECRET_ENCRYPTION_KEY_HERE

# --- Docker & Container Configuration ---
# Timeout for container health checks (milliseconds)
CONTAINER_HEALTH_CHECK_TIMEOUT=30000

# Default health check path for deployed apps
HEALTH_CHECK_PATH=/healthz

# --- Build & Deployment Commands ---
# Override default build/start commands per project (can also be set per-project in DB)
DEFAULT_BUILD_COMMAND=npm run build
DEFAULT_START_COMMAND=npm start

# --- Rate Limiting & Performance ---
# Max concurrent LLM requests (to avoid API throttling)
MAX_CONCURRENT_LLM_CALLS=5

# Request timeout in seconds for LLM API calls
LLM_REQUEST_TIMEOUT=120

# --- Feature Flags ---
# Enable/disable specific platform features
ENABLE_AUTO_APPLY=true
ENABLE_DEPLOYMENT_MONITORING=true
ENABLE_CROSS_LLM_VALIDATION=true
