# Project GenCode Backend - Complete Function Analysis
# Generated: December 24, 2025 (Programmatic Extraction)

Total Functions: 875
Total Classes: 169

================================================================================

# PART 1: CLASSES (169 total)
================================================================================

## app/api/deployment.py

- **DeploymentInitRequest** (line 17)
- **DeploymentRequest** (line 24)
- **EnvVarsRequest** (line 30)
- **DomainRequest** (line 34)

## app/api/projects.py

- **CreateProjectRequest** (line 21)
- **ProjectResponse** (line 27)

## app/api/providers.py

- **ProviderInfo** (line 14)
- **SetProviderRequest** (line 106)

## app/api/sandbox.py

- **ExecRequest** (line 12)

## app/api/workspace.py

- **FileContent** (line 61)
- **GenerateRequest** (line 66)
- **ResumeRequest** (line 73)
- **ApplyInstructionRequest** (line 355)

## app/arbormind/cognition/branch.py

- **BranchStatus** (line 8)
- **ExecutionState** (line 14)
- **Branch** (line 21)

## app/arbormind/cognition/convergence.py

- **ConvergenceState** (line 18): The cognitive state of a branch's progress.  This is ORTHOGONAL to success/failure. A branch can fail but be converging (making progress). A branch ca

## app/arbormind/cognition/execution_report.py

- **ExecutionReport** (line 4)

## app/arbormind/cognition/failures.py

- **FailureSeverity** (line 19): How severely a failure affects branch execution.  NON_FATAL: Branch can continue with recovery/mutation FATAL: Branch must terminate
- **InvariantViolation** (line 77): Structured failure result from invariant checks.  IMPORTANT: This is NOT an exception. Invariants RETURN violations, they don't RAISE them. The orches

## app/arbormind/cognition/lineage.py

- **LineageType** (line 26): What kind of artifact lineage this is.
- **ArtifactLineage** (line 38): IMMUTABLE binding between Artifact → Branch → Action → Agent.  This is the provenance chain that enables learning.  frozen=True ensures immutability a
- **LineageRegistry** (line 181): In-memory registry for artifact lineages.  Provides fast lookup for: - Artifacts by branch - Artifacts by agent - Artifacts by action type - Parent-ch

## app/arbormind/cognition/tree.py

- **ArborMindTree** (line 5)

## app/arbormind/core/archetypes.py

- **Archetype** (line 17): Immutable archetype template. Safe to copy into Branch.assumptions.

## app/arbormind/core/epistemic_guard.py

- **EpistemicBoundaryViolation** (line 52): Raised when code attempts to cross the epistemic boundary. This is a HARD failure - not recoverable.

## app/arbormind/core/execution_mode.py

- **ExecutionMode** (line 14): Execution modes define HOW a step should execute.  - ARTIFACT: Must produce HDAP-formatted files, strict parsing - FREEFORM: Free-form output, minimal
- **ExecutionPolicy** (line 29): Execution policy defines WHAT to do when a step fails.  Separate from ExecutionMode (which defines HOW to execute).  Phase-0/1 Fields: - mode: How the

## app/arbormind/core/tool_selector.py

- **SelectedTool** (line 18)

## app/arbormind/invariants/invariant.py

- **InvariantViolation** (line 21): DEPRECATED: Use StructuredViolation instead.  This exception is kept for backward compatibility but should be phased out. New code should use Invarian
- **Invariant** (line 32): A structural guardrail that returns violations instead of crashing.  INVARIANTS: - check() returns Optional[StructuredViolation], never raises - Orche

## app/arbormind/memory/pattern_store.py

- **PatternStore** (line 18)

## app/arbormind/observation/execution_ledger.py

- **EpistemicViolation** (line 133)
- **ExecutionLedger** (line 136): Append-only event recorder. Ignorant of semantics.

## app/arbormind/observation/failure_canon.py

- **FailureClass** (line 29): The 9 canonical failure classes.  Referenced by ID (e.g., F1, F2), not by text. Each has exactly one semantic meaning.
- **FailureScope** (line 93): REQUIREMENT 5: Explicit Scope Assignment  Every failure MUST declare where truth lives. No inference later. No retroactive correction.
- **FailureClassDefinition** (line 115): Immutable definition of a failure class.  These are the canonical definitions. They cannot be changed at runtime.

## app/arbormind/observation/failure_ontology.py

- **ExecutionLayer** (line 51): Where in the execution stack did the failure occur?  This axis replaces the overloaded meaning currently collapsed into F7.
- **AuthorityBoundary** (line 86): Who had authority over the action that failed?  Authority is NEVER inferred from failure content. It must be known at invocation time.
- **GatingSemantics** (line 117): Does this failure affect run success semantics?  THIS IS THE CRITICAL AXIS that resolves the paradox:     "SUCCESS run with HARD failures"  The parado
- **TruthDomain** (line 154): What kind of truth was violated?  This prevents validators from masquerading as execution failures. Observation truth ≠ Execution truth.
- **TemporalPosition** (line 191): When did the failure occur relative to artifact materialization?  This is critical for distinguishing:     - "Nothing was created"     - "Something wa
- **ArtifactImpact** (line 220): What did the failure affect in terms of artifacts?  Artifact validity is NOT inferred from this axis. This is observational only.
- **RepeatabilitySignature** (line 251): Is this failure stable/repeatable?  This is observational only — it does NOT enable retries. It exists for future learning (Phase-4) to avoid false pa
- **FailureOntologyClassification** (line 281): Complete 7-axis classification of a failure.  Immutable once created. All axes are mandatory for new classifications.

## app/arbormind/observation/failure_record.py

- **FailureRecord** (line 31): A single failure record.  This is the canonical shape of a failure. All fields are required at construction time.  v3: Includes 7-axis ontology classi

## app/arbormind/observation/failure_semantics.py

- **SemanticClass** (line 44): Ontologically sound semantic failure classes.  Each class lives in a different causal plane: - EXECUTION: Tool ran but crashed - TOPOLOGY: Tool was in
- **SemanticClassification** (line 120): Result of classifying a failure.

## app/arbormind/observation/interpretation_context.py

- **SignalExtractorContext** (line 53): Frozen snapshot of signal extractor rules.  If the regex patterns or extraction logic changes, the hash changes.
- **CanonContext** (line 99): Frozen snapshot of failure class definitions.  If the meaning of F1-F9 changes, the hash changes.
- **ScopeContext** (line 135): Frozen snapshot of scope semantics.  If the meaning of ENTITY_LOCAL, STEP_LOCAL, etc. changes, the hash changes.
- **InterpretationContext** (line 174): Complete frozen interpretation context.  This is what gets stored with each failure to guarantee temporal truth invariance.

## app/arbormind/observation/observer.py

- **ArborMindObserver** (line 23): Passive observer that records execution events.  This class has ZERO cognitive authority. It only watches and records. Learning happens offline, separ

## app/arbormind/observation/run_event.py

- **RunEvent** (line 14): Immutable record of a single execution event.  Attributes:     run_id: Unique identifier for the entire workflow run     branch_id: Unique identifier 

## app/arbormind/observation/signal_extractor.py

- **SignalType** (line 34): Types of atomic signals that can be extracted.  Each type has exactly one extraction rule.
- **AtomicSignal** (line 72): A single, atomic, immutable signal.  Properties: - Hashable (can be counted) - Immutable (frozen dataclass) - Small (just type + value + optional cont
- **SignalExtractionResult** (line 107): Result of signal extraction.  Contains: - List of atomic signals (ordered by extraction) - Deduped set for counting - Hash of the input for reproducib
- **SignalExtractor** (line 139): Dumb signal extractor.  No heuristics. No interpretation. Just pattern matching.  Same input → same output. Always.

## app/arbormind/observation/step_state_snapshot.py

- **StepStateSnapshot** (line 42): Immutable workspace state at a moment in time.  No semantics. Just facts.

## app/arbormind/observation/tool_trace.py

- **ToolInvocationEvent** (line 105): Single tool invocation record.  Immutable. No defaults. If data is missing → pass None.

## app/arbormind/priors/tool_priors.py

- **ToolPriors** (line 12): Static configuration of tool selection biases.  This is NOT a live query system. Priors are loaded once at startup or updated infrequently (e.g., dail

## app/arbormind/reconstruction/run_slice_builder.py

- **RunSlice** (line 20): Reconstructed state of a run at a point in time. Immutable representation of 'What Happened'.
- **RunSliceBuilder** (line 31)

## app/arbormind/runtime/actions.py

- **ActionType** (line 16): What the system is trying to accomplish.  This is orthogonal to specific step names.

## app/arbormind/runtime/decision.py

- **ExecutionDecision** (line 6)
- **ExecutionAction** (line 16)

## app/arbormind/runtime/execution_router.py

- **ExecutionRouter** (line 15): Central decision authority for execution.  INVARIANTS: - Exactly ONE decision per invocation - No retries, loops, or side-effects - Agents never choos

## app/arbormind/runtime/runtime.py

- **ArborMindRuntime** (line 9): Phase 8 Runtime (Updated with Phase 5 Memory)  Responsibilities: - Expand hypothesis space (Divergence) - Respond to failure (Mutation / Healing) - Me

## app/arbormind/storage/run_ledger.py

- **RunLedger** (line 63): Append-only JSONL storage for execution events.  Each line is a complete JSON object representing one RunEvent. This design is intentionally simple fo

## app/core/config.py

- **LLMSettings** (line 15): LLM provider configuration.
- **WorkflowSettings** (line 28): Workflow execution configuration.
- **SandboxSettings** (line 41): Docker sandbox configuration.
- **PathSettings** (line 49): Path configuration.
- **AMSettings** (line 67): ArborMind (AM) configuration.  Controls the creative reasoning operators: - C-AM: Combinational (blend multiple archetypes) - E-AM: Exploratory (injec
- **Settings** (line 103): Main application settings.

## app/core/constants.py

- **WorkflowStep** (line 7): GenCode Studio workflow steps (Frontend-First pattern).  Order (9 steps total - Atomic Backend): 1. ARCHITECTURE - Design system architecture 2. FRONT
- **AgentName** (line 53): Agent identifiers.
- **WSMessageType** (line 97): WebSocket message types.

## app/core/exceptions.py

- **GenCodeError** (line 8): Base exception for all GenCode errors.
- **WorkflowError** (line 16): Workflow execution error.
- **QualityGateError** (line 21): Quality gate blocked the workflow.
- **AgentError** (line 33): Agent execution error.
- **LLMError** (line 44): LLM provider error.
- **RateLimitError** (line 54): Rate limit exhausted - workflow should STOP immediately.
- **SandboxError** (line 65): Docker sandbox error.
- **PersistenceError** (line 75): File persistence error.
- **ValidationError** (line 85): Output validation error.
- **ParseError** (line 90): JSON/output parsing error.

## app/core/execution_record.py

- **StepExecutionRecord** (line 14): Record of what a step produced during execution.  Used for: - Rollback (delete files created by failed step) - Resume (know what each step created) - 

## app/core/failure_boundary.py

- **FailureBoundary** (line 21): Enforces classification boundary.  This decorator ensures that: 1. No handler can return raw "failed" status 2. All exceptions are caught and classifi
- **LegacyStepResultConverter** (line 149): Helper to convert legacy StepResult objects to new StepExecutionResult.  Use this during migration phase when you need to manually convert results.

## app/core/failure_reporting.py

- **FailureType** (line 32): Canonical failure types for honest reporting.

## app/core/guard.py

- **OrchestrationGuard** (line 8): Safety lock to prevent accidental re-introduction of cognitive decisions (retries, escalation, healing) into the orchestration layer.

## app/core/integration.py

- **IntegrationConfig** (line 37): Configuration for integration features
- **IntegrationManager** (line 49): Manages feature flags and configuration for GenCode Studio. Ported from legacy.

## app/core/llm_output_integrity.py

- **LLMOutputIntegrityError** (line 9)

## app/core/step_invariants.py

- **StepInvariantError** (line 16): Raised when a step invariant is violated.
- **StepInvariants** (line 21): Unified invariant checker for all workflow steps.  Usage:     StepInvariants.require_files(parsed, step_name, min_files=1)     StepInvariants.require_

## app/core/step_outcome.py

- **StepOutcome** (line 15): Step-level outcomes (4 types only).
- **StepExecutionResult** (line 24): Result of a single step execution.

## app/core/types.py

- **ChatMessage** (line 11)
- **GeneratedFile** (line 21): A file generated by an agent.
- **AgentOutput** (line 33): Output from an agent call.
- **StepResult** (line 51): Result of executing a workflow step.
- **QualityMetrics** (line 64): Quality metrics for a project.
- **TokenUsage** (line 75): Track token usage for cost estimation.
- **WorkflowStatus** (line 93): Workflow-level outcomes (aggregate property).  Phase 1: Updated to support degradation reporting.
- **DegradationReport** (line 110): Report for workflows that complete with degradation.  Phase 1: This is an AGGREGATE property, not step-level. Degradation means some steps were isolat
- **QAIssue** (line 133): Quality assurance issue from Derek/Luna.
- **FilePlan** (line 145): File plan for workflow.
- **TestReport** (line 151): Test report from QA agents.
- **ToolCall** (line 161): A tool call from Marcus.
- **MarcusPlan** (line 167): Marcus's planning output.

## app/lib/secrets.py

- **EncryptedSecret** (line 17)

## app/lib/websocket.py

- **ConnectionManager** (line 7): Simple per-project WebSocket connection manager.  - Each project_id has its own list of WebSocket connections. - You can send to one socket, all socke

## app/llm/adapter.py

- **LLMAdapter** (line 72): Unified adapter for LLM providers.  Handles: - Provider selection - SINGLE EXECUTION (ArborMind handles retry via branch continuation) - Rate limit ha

## app/models/deployment.py

- **Deployment** (line 8): Deployment configuration and status.
- **Settings** (line 29)

## app/models/project.py

- **Project** (line 11)
- **Settings** (line 20)

## app/models/snapshot.py

- **Snapshot** (line 6)
- **Settings** (line 15)

## app/models/workflow.py

- **WorkflowStepRecord** (line 6)
- **Settings** (line 14)
- **WorkflowSession** (line 18): Persisted state of a workflow session. Replaces in-memory _running_workflows, _paused_workflows, etc.
- **Settings** (line 46)

## app/orchestration/backend_probe.py

- **ProbeMode** (line 18): Backend probe execution mode.
- **BackendProbe** (line 25): Abstract backend probe interface.
- **HTTPBackendProbe** (line 66): HTTP-based backend probe for Docker/Sandbox environments.
- **ASGIBackendProbe** (line 187): ASGI-based backend probe using ASGITransport (like conftest.py).

## app/orchestration/budget_manager.py

- **StepPolicy** (line 38): Per-step budget policy.  - skippable: if True, step may be skipped when budget is tight - max_attempts: hard cap on LLM retries for this step - est_in
- **BudgetConfig** (line 54): Global budget config for a single FAST run, using: - one model: Gemini 2.5 Flash - currency: INR
- **BudgetManager** (line 125): Cost controller for a single FAST run (Gemini 2.5 Flash only).  - Tracks real usage in USD/INR based on actual token counts - Decides how many attempt

## app/orchestration/checkpoint.py

- **CheckpointManagerV2** (line 17): Stores SAFE checkpoints of each FAST step. Does NOT store broken artifacts.

## app/orchestration/context.py

- **CrossStepContext** (line 227): V2 Feature: Maintains context summaries across workflow steps.  This helps later steps understand what earlier steps produced, reducing errors and imp

## app/orchestration/fast_orchestrator.py

- **FASTOrchestratorV2** (line 91): FAST v2 Orchestrator - Minimal execution muscle.

## app/orchestration/file_persistence.py

- **FilePersistence** (line 15): Handles all writes to disk for FAST v2. Ensures consistency, avoids partial writes using atomic rename.

## app/orchestration/llm_output_integrity.py

- **LLMOutputIntegrityError** (line 9)

## app/orchestration/state.py

- **WorkflowStateManager** (line 19): Manages workflow state using persistent storage (MongoDB).

## app/orchestration/structural_compiler.py

- **StructuralViolation** (line 7)
- **StructuralCompiler** (line 11): Authoritative structural validator. If this fails, the branch MUST fail.

## app/orchestration/task_graph.py

- **TaskGraph** (line 13): Defines the fixed FAST v2 pipeline, but allows adaptive behavior INSIDE each step (hybrid adaptive mode). Steps cannot be removed, added, or reordered

## app/sandbox/health_monitor.py

- **HealthMonitor** (line 13): Monitors health of sandbox containers via `docker inspect`.

## app/sandbox/log_streamer.py

- **LogStreamer** (line 12): Streams container logs in real-time using `docker logs -f`.

## app/sandbox/pool.py

- **PooledSandbox** (line 28): A pre-warmed sandbox ready for use.
- **SandboxPool** (line 37): Manages a pool of pre-warmed Docker containers.  Benefits: - 49s → 5s startup time - No cold start delays - Async pool refill

## app/sandbox/preview_manager.py

- **PreviewManager** (line 9): Manages public preview URLs for sandboxes via Traefik

## app/sandbox/sandbox_config.py

- **SandboxConfig** (line 11): Configuration for a Docker sandbox environment

## app/sandbox/sandbox_manager.py

- **SandboxManager** (line 22)

## app/supervision/tiered_review.py

- **ReviewLevel** (line 20): Review levels in order of intensity.

## app/tools/implementations.py

- **GenCodeTool** (line 170)

## app/tools/migration.py

- **ObservedHandler** (line 173): Wraps an existing handler to add tool observation.  Usage:     HANDLERS = {         "backend_models": ObservedHandler(step_backend_models, "backend_mo

## app/tools/patching.py

- **PatchEngine** (line 160)

## app/tools/planner.py

- **ToolPlanBuilder** (line 50): Builds binding tool plans for steps.  The Flow: 1. Step name → Get tools for phase 2. Order by: pre-step → core → post-step 3. Build ToolPlan  NO LLM 

## app/tools/planning.py

- **ToolInvocationPlan** (line 23): A single tool invocation in a plan.  Immutable. Observable. Non-executing.
- **ToolPlan** (line 48): A complete execution plan for a step.  This is the ONLY new concept you need.  Properties: - Immutable: Cannot be modified after creation - Observable
- **ToolInvocationResult** (line 96): Result of executing a single tool invocation.
- **ToolPlanExecutionResult** (line 123): Result of executing a complete tool plan.
- **StepFailure** (line 156): Raised when a required tool in a plan fails.

## app/tools/tools.py

- **Capability** (line 29): What a tool CAN DO.
- **ToolDefinition** (line 93): Complete definition of a tool.

## app/utils/entity_discovery.py

- **Field** (line 26): Field specification for an entity.
- **Relationship** (line 47): Relationship between entities.
- **EntitySpec** (line 68): Complete specification for an entity.
- **EntityPlan** (line 105): Complete entity generation plan.

## app/validation/static_validator.py

- **StaticValidationEvidence** (line 19): Evidence collected from static analysis.  Philosophy (ADJUSTMENT 2): - We OBSERVE facts - We EMIT evidence - We DO NOT make verdicts - The aggregator 
- **StaticValidator** (line 56): Provides static evidence about code without executing it.  Use cases: 1. When step is isolated due to ENVIRONMENT_FAILURE 2. When we want to validate 

## app/validation/syntax_validator.py

- **ValidationResult** (line 22): Result of syntax validation.
- **IncompleteCodeError** (line 48)


================================================================================
# PART 2: FUNCTIONS (875 total)
================================================================================

## app/agents/sub_agents.py

- `async _broadcast_agent_thinking(project_id, agent_name, status, content)` (line 31): Broadcast agent thinking to the frontend Terminal.
- `async _llm_generate_tests(user_request, project_path, agent_name, provider, model, ...)` (line 56): Ask LLM to generate tests given the current project files. Returns parsed JSON { "tests": [{"path": "...", "content": "..."}], "metadata": {...} }
- `_collect_files_sync()` (line 83)
- `_write_tests_to_workspace(project_path, tests)` (line 160): Persist generated tests into workspace and return list of written paths.
- `async _run_pytest(project_path, test_paths, timeout)` (line 176): Run pytest programmatically via async subprocess. Returns structured dict: passed(bool), failures(list), output(str)
- `run_pytest_sync()` (line 186)
- `async _run_playwright(project_path, test_paths, timeout)` (line 219): Try to run playwright tests via CLI 'playwright test' (async). Returns structured dict similar to pytest runner.
- `run_playwright_sync()` (line 229)
- `async run_sub_agent(name, user_request, project_path, provider, model, ...)` (line 259): Generic runner for a sub-agent: - Ask LLM to produce tests - Persist them - Attempt to run them - Return structured report
- `async marcus_call_sub_agent(agent_name, user_request, project_path, project_id, step_name, ...)` (line 324): Marcus uses this to call Derek/Luna/Victoria for code generation.  Uses full agent prompts with HDAP format instructions.

## app/api/agents.py

- `async get_agents_status()` (line 13): Get status of all agents.
- `async get_active_workflows()` (line 42): Get list of active workflows.

## app/api/deployment.py

- `async initialize_deployment(data)` (line 39): Initialize a new deployment.
- `async start_deployment(project_id, data)` (line 67): Start one-click deployment.
- `async get_deployment_status(project_id)` (line 95): Get deployment status.
- `async get_deployment_history(project_id, limit)` (line 118): Get deployment history.
- `async rollback_deployment(project_id)` (line 137): Rollback to previous version.
- `async update_env_vars(project_id, data)` (line 152): Update environment variables.
- `async get_env_vars(project_id)` (line 169): Get environment variables.
- `async setup_custom_domain(project_id, data)` (line 179): Setup custom domain.
- `async get_deployment_logs(project_id)` (line 197): Get deployment logs.
- `async download_logs(project_id)` (line 209): Download deployment logs as file.
- `async restart_deployment(project_id)` (line 221): Restart deployment.
- `async get_deployment_metrics(project_id)` (line 232): Get deployment metrics.
- `async get_container_health(project_id)` (line 243): Get container health status.

## app/api/health.py

- `async healthz()` (line 12): Simple health check.
- `async api_health()` (line 18): API health check.

## app/api/projects.py

- `async create_project(request, data)` (line 40): Create a new project.   NOTE: This endpoint only creates the project directory. The workflow is started separately via /api/workspace/{id}/generate/ba
- `generate_slug(prompt)` (line 51): Generate a URL-safe slug from the user prompt.
- `async list_projects()` (line 129): List all projects.
- `async get_project(project_id)` (line 179): Get project details.
- `async delete_project(project_id)` (line 226): Delete a project.

## app/api/providers.py

- `async list_providers()` (line 21): List available LLM providers.
- `async get_available_providers()` (line 54): Get list of available providers (those with API keys configured).
- `async get_current_provider()` (line 98): Get current default provider and model.
- `async set_provider(data)` (line 116): Set the current provider and model.

## app/api/sandbox.py

- `async sandbox_exec(data)` (line 20): Execute command in sandbox.
- `async get_sandbox_status(project_id)` (line 39): Get sandbox status for a project.
- `async create_sandbox(project_id)` (line 53): Create/initialize sandbox for a project (idempotent).
- `async start_sandbox(project_id, wait_healthy)` (line 77): Start sandbox for a project.
- `async stop_sandbox(project_id)` (line 91): Stop sandbox for a project.
- `async get_preview_url(project_id)` (line 105): Get preview URL for a project sandbox.

## app/api/tracking.py

- `async health()` (line 16): Health check for tracking service.
- `async get_project_costs(project_id)` (line 22): Get cost/token usage data for a project.  Returns data in the format expected by CostDashboard.tsx: - total_input_tokens, total_output_tokens, total_t
- `async get_project_budget(project_id)` (line 38): Get budget status for a project.  Returns current budget status including: - used_inr, remaining_inr, max_inr - status (HEALTHY, MODERATE, LOW, CRITIC
- `async list_tracked_projects()` (line 52): List all projects with budget tracking.

## app/api/workspace.py

- `validate_project_id(project_id)` (line 29): Validate project_id format to prevent path traversal attacks. Only allows alphanumeric, hyphens, and underscores (1-100 chars).
- `get_safe_project_path(project_id)` (line 39): Get project path with security validation. Raises HTTPException if project_id is invalid.
- `async list_workspaces()` (line 78): List all workspaces (for connection test).
- `async get_workspace_files(project_id)` (line 92): Get workspace file tree.
- `build_tree(path)` (line 99)
- `async get_file_content(project_id, path)` (line 128): Get file content.
- `async save_file_content(project_id, data)` (line 158): Save file content.
- `async generate_backend(request, project_id, data)` (line 182): Start backend generation workflow.  Resume Modes: - "auto": Check for saved progress, resume if found, else start fresh - "resume": Force resume (fail
- `async _resume_workflow_with_logging()` (line 245)
- `async _run_workflow_with_logging()` (line 273)
- `async resume_workflow_endpoint(request, data)` (line 297): Resume a paused workflow OR start a refine workflow for completed projects.  Delegates to the consolidated engine.resume_workflow which handles: - Res
- `async apply_instruction(project_id, data)` (line 360): Apply an instruction to modify project files.
- `async force_reset_workflow(project_id)` (line 373): Force reset workflow state for a project. Use this if a workflow crashed and left stale state.
- `async get_project_progress(project_id)` (line 391): Get workflow progress for a project.  Returns: - completed_steps: List of steps that have been completed - current_step: The last completed step - is_
- `async clear_project_progress(project_id)` (line 429): Clear all saved progress for a project. Use this to force a fresh start on next generation.
- `async force_stop_workflow(project_id)` (line 450): Force stop a stuck workflow.  Use this when a workflow is marked as running but is actually stuck/crashed. This manually clears the is_running flag.

## app/arbormind/cognition/branch.py

- `__init__(self, parent_id, depth, assumptions, intent, ...)` (line 22)
- `project_path(self)` (line 70): Extract project_path from intent for executor access.
- `project_id(self)` (line 75): Extract project_id from intent.
- `workspace_context(self)` (line 80): Return workspace context for tools.

## app/arbormind/cognition/convergence.py

- `is_converged(branch, completed_steps)` (line 33): Check if a branch has converged (ready for completion).  PHASE 6: Convergence criteria: 1. Has core architecture 2. Has frontend (mock or integrated) 
- `get_completion_confidence(branch, completed_steps)` (line 66): Calculate completion confidence (0.0 - 1.0).  Higher confidence = more optional steps completed. Tests increase confidence but don't block completion.
- `should_preview(branch, completed_steps)` (line 107): Check if a branch is ready for preview.  PHASE 6: Preview is allowed earlier. Preview no longer gated on tests. Tests increase confidence, not complet
- `get_missing_for_convergence(completed_steps)` (line 130): Get list of steps missing for convergence.  Useful for guiding next action selection.

## app/arbormind/cognition/divergence.py

- `generate_child_branches(parent, variants)` (line 5): PURE helper. Turns variant descriptors into child Branch objects.

## app/arbormind/cognition/entropy.py

- `estimate_entropy(branch, context)` (line 10)
- `update_entropy(branch, new_entropy)` (line 44): Update branch entropy and convergence state based on direction.  This is KEY: we track direction, not just outcome. - Entropy decreasing = CONVERGING 

## app/arbormind/cognition/execution_report.py

- `__init__(self, success, artifacts, error, metrics, ...)` (line 5)

## app/arbormind/cognition/failures.py

- `get_failure_severity(failure_code)` (line 61): Get the severity of a failure code.  Unknown failures are treated as NON_FATAL by default (prefer continuation over termination).
- `is_fatal(failure_code)` (line 71): Check if a failure code is fatal.
- `__post_init__(self)` (line 90): Auto-classify severity from code if not provided.
- `is_fatal(self)` (line 95): Check if this violation is fatal.
- `to_dict(self)` (line 99): Convert to dictionary for logging/storage.

## app/arbormind/cognition/lineage.py

- `__hash__(self)` (line 87): Hash based on artifact_id for set operations.
- `to_dict(self)` (line 91): Convert to dictionary for storage.
- `from_dict(cls, data)` (line 113): Create from dictionary.
- `generate_artifact_id(path, branch_id, content_hash)` (line 118): Generate a unique artifact ID from path, branch, and content.  Format: art_{path_hash}_{branch_hash}_{content_hash[:8]}
- `compute_content_hash(content)` (line 129): Compute SHA-256 hash of content.
- `create_lineage(artifact_path, content, branch_id, branch_depth, action_type, ...)` (line 134): Create an immutable artifact lineage binding.  This is the FACTORY function for creating lineages.
- `__init__(self)` (line 192)
- `register(self, lineage)` (line 199): Register a lineage (immutable once registered).
- `get(self, artifact_id)` (line 227): Get lineage by artifact ID.
- `get_by_branch(self, branch_id)` (line 231): Get all artifacts produced by a branch.
- `get_by_agent(self, agent_name)` (line 236): Get all artifacts produced by an agent.
- `get_by_action(self, action_type)` (line 241): Get all artifacts produced by an action type.
- `get_file_history(self, path)` (line 246): Get all versions of a file (by path).
- `get_lineage_chain(self, artifact_id)` (line 251): Get the full lineage chain from artifact back to root.  Returns: [current, parent, grandparent, ..., root]
- `count_by_branch(self, branch_id)` (line 269): Count artifacts in a branch.
- `count_by_agent(self, agent_name)` (line 273): Count artifacts by agent.
- `get_lineage_registry()` (line 282): Get the global lineage registry.
- `register_artifact_lineage(lineage)` (line 290): Register an artifact lineage in the global registry.
- `get_artifacts_for_branch(branch_id)` (line 295): Get all artifacts for a branch.
- `get_artifacts_for_agent(agent_name)` (line 300): Get all artifacts for an agent.
- `get_branch_artifact_count(branch_id)` (line 305): Get artifact count for a branch.

## app/arbormind/cognition/partial_output.py

- `allows_partial_output(step_name)` (line 49): Check if a step allows partial output.
- `evaluate_progress(step_name, artifacts_count, expected_count)` (line 54): Evaluate if output represents progress.  Returns (is_progress, reason)  PHASE 4 KEY CHANGE: Any artifacts > 0 is progress if step allows partial.
- `should_mark_success(step_name, artifacts_count, quality_score, previous_artifacts_count)` (line 78): Determine if a step should be marked as successful.  PHASE 4 CHANGE: - artifacts_count > 0 = mark_progress() - Quality is directional (did we improve?
- `calculate_entropy_delta(artifacts_before, artifacts_after, failures_before, failures_after)` (line 110): Calculate entropy change based on artifacts and failures.  Negative = entropy decreased (good) Positive = entropy increased (bad)

## app/arbormind/cognition/tree.py

- `__init__(self, root)` (line 6)
- `add_branch(self, branch)` (line 10)
- `get_branch(self, branch_id)` (line 13)
- `active_branches(self)` (line 16)
- `dormant_branches(self)` (line 22)

## app/arbormind/core/archetypes.py

- `__init__(self, name, description, default_assumptions, constraints, ...)` (line 23)
- `as_assumptions(self)` (line 35): Convert archetype into branch-safe assumptions.
- `get_archetype(name)` (line 144): Lookup helper. No defaults, no fallback decisions.

## app/arbormind/core/epistemic_guard.py

- `assert_not_cognition_data(table_name)` (line 60): Assert that a table is not cognition data. Call this before any 'truth extraction' operation.
- `assert_no_forbidden_columns(column_names)` (line 72): Assert that no forbidden columns are being accessed.
- `print_boundary_info()` (line 114): Print boundary documentation to stdout.

## app/arbormind/core/execution_mode.py

- `__init__(self, mode, is_fatal, allow_retry, max_retries, ...)` (line 42)
- `is_telemetry_only_tool(tool_name)` (line 175): Check if a tool is telemetry-only (observation, not enforcement).
- `get_execution_policy(step_name)` (line 180): Get the execution policy for a step.  SOURCE OF TRUTH for step execution behavior.
- `should_enforce_hdap(step_name)` (line 206): Quick check: Does this step require HDAP enforcement?
- `is_step_fatal(step_name)` (line 214): Quick check: Should failure in this step halt the workflow?
- `is_generation_step(step_name)` (line 251): Check if this step generates files (uses LLM).
- `is_execution_step(step_name)` (line 257): Check if this step is execution-only (no LLM generation).
- `requires_runtime(step_name)` (line 263): Check if this step requires runtime to be running.
- `should_skip_if_runtime_down(step_name)` (line 269): PHASE B2: Steps that should skip execution if runtime is not running. Returns True for tools that depend on live server.

## app/arbormind/core/explorer.py

- `generate_variants(branch)` (line 11): Phase 4 adapter. Returns shallow variants WITHOUT decisions.

## app/arbormind/core/lockfile.py

- `_get_lockfile_path(project_path)` (line 35): Get the lockfile path for a project.
- `_compute_input_hash(step_name, user_request, contracts, files)` (line 40): Compute a deterministic hash of inputs for a step.  This hash changes when: - User request changes - Architecture/contracts change - Input files chang
- `load_lockfile(project_path)` (line 71): Load the lockfile for a project.
- `save_lockfile(project_path, lockfile)` (line 92): Save the lockfile for a project.
- `check_cache_hit(project_path, step_name, user_request, contracts, files, ...)` (line 106): Check if we have a valid cache hit for this step.  Returns:     - Cached result dict if hit (with files, artifacts)     - None if miss (need to run LL
- `record_step_completion(project_path, step_name, user_request, files_written, contracts, ...)` (line 147): Record a step completion in the lockfile.  Called after LLM successfully generates output.
- `invalidate_step(project_path, step_name)` (line 175): Invalidate cache for a specific step.
- `invalidate_all(project_path)` (line 185): Invalidate all cached steps (full rebuild).
- `get_lockfile_summary(project_path)` (line 194): Get a summary of cached steps for debugging.
- `should_skip_generation(project_path, step_name, user_request, contracts, files, ...)` (line 212): Check if this generation step should be skipped (cache hit).  Returns:     (should_skip, cached_result)

## app/arbormind/core/retry_policy.py

- `get_retry_prompt(step_name)` (line 114): Get hardened retry prompt for a step.  Args:     step_name: Step that failed (e.g., "backend_models")      Returns:     Hardened retry prompt, or gene
- `has_retry_prompt(step_name)` (line 142): Check if a step has a defined retry prompt.

## app/arbormind/core/synthesis.py

- `synthesize(tree)` (line 5)

## app/arbormind/core/t_am_operators.py

- `relax_constraints(config)` (line 34)
- `expand_edit_scope(config)` (line 43)
- `downgrade_expectations(config)` (line 52)
- `relax_architecture_constraints(assumptions)` (line 63)
- `narrow_problem_scope(assumptions)` (line 69)
- `switch_testing_strategy(assumptions)` (line 75)
- `mutate_archetype(assumptions)` (line 81)

## app/arbormind/core/tool_selector.py

- `select_tool(branch, context, entropy)` (line 23): Tool Attention Mechanism (TAM)  RULES: - Agents NEVER choose tools - Selection is deterministic - Priors only bias, never override

## app/arbormind/invariants/invariant.py

- `__init__(self, name, detector, failure_code, message_template, ...)` (line 42)
- `check(self, branch)` (line 54): Check invariant and return violation if detected.  Returns None if invariant holds. Returns StructuredViolation if invariant is violated.  NEVER raise
- `enforce(self, branch)` (line 81): LEGACY: Raise exception if invariant violated.  DEPRECATED: Use check() instead. This method is kept for backward compatibility only.
- `check_all_invariants(invariants, branch)` (line 93): Check all invariants and return list of violations.  This is the cognitive way: collect all problems, then decide.
- `has_fatal_violation(violations)` (line 110): Check if any violation is fatal.

## app/arbormind/memory/consolidator.py

- `consolidate_tree(tree)` (line 12): Compress branch histories into global patterns.

## app/arbormind/memory/pattern_store.py

- `__init__(self)` (line 19)
- `record_failure(self, archetype)` (line 25)
- `record_success(self, archetype)` (line 28)
- `record_healing(self, healing_action)` (line 31)
- `record_mutation(self, mutation)` (line 34)

## app/arbormind/observation/execution_ledger.py

- `__init__(self)` (line 145)
- `get_instance(cls)` (line 151)
- `_get_conn(self)` (line 158)
- `_cursor(self)` (line 168)
- `_init_db(self)` (line 180)
- `record_run_start(self, run_id, project_id)` (line 188)
- `record_step_entry(self, run_id, step)` (line 195)
- `record_step_exit(self, run_id, step, status)` (line 202)
- `record_decision_event(self, run_id, step, agent, event_type, ...)` (line 210)
- `record_failure_event(self, run_id, step, origin, signal, ...)` (line 217)
- `record_supervisor_event(self, run_id, step, agent, payload_json, ...)` (line 224)
- `record_tool_trace(self, run_id, step, tool_name, input_hash, ...)` (line 231)
- `record_artifact_event(self, run_id, step, file_path, event_type, ...)` (line 238): Record artifact birth/modification event at the file materialization boundary.
- `record_snapshot(self, run_id, step, stage, workspace_hash, ...)` (line 246)
- `_dump_table(self, table_name, run_id)` (line 257): Raw table dump for builder. No logic.
- `set_current_run_id(run_id)` (line 271)
- `get_current_run_id()` (line 275)
- `get_store()` (line 278)
- `record_run_start(run_id, project_id)` (line 282)
- `record_step_entry(run_id, step)` (line 285)
- `record_step_exit(run_id, step, status)` (line 288)
- `record_decision_event(run_id, step, agent, decision, reason, ...)` (line 291)
- `record_failure_event(run_id, step, origin, message, signal, ...)` (line 297)
- `record_supervisor_event(run_id, step, agent, verdict, quality, ...)` (line 300)
- `record_snapshot(run_id, step, stage, workspace_hash, artifacts_hash, ...)` (line 304)
- `record_artifact_event(run_id, step, file_path, event_type, size_bytes, ...)` (line 307): Record artifact birth/modification at materialization boundary.
- `record_tool_trace(run_id, step, tool_name, input_hash, exit_code, ...)` (line 311): Record tool invocation trace for cost/duration attribution.
- `update_decision_outcome(run_id, step, outcome, duration_ms, artifacts_count, ...)` (line 317)
- `record_failure(run_id, step, failure_type, message)` (line 321)

## app/arbormind/observation/failure_canon.py

- `get_failure_class(class_id)` (line 198): Get FailureClass by ID string (e.g., "F1").  Returns None if not found (illegal ID).
- `get_definition(fc)` (line 210): Get the canonical definition for a failure class.
- `validate_class_id(class_id)` (line 215): Check if a class ID is legal.

## app/arbormind/observation/failure_ontology.py

- `is_blocking(self)` (line 314): Should this failure stop the run?
- `is_observation_only(self)` (line 318): Is this purely observational (no execution impact)?
- `explains_hard_success_paradox(self)` (line 326): Does this classification explain a HARD failure in a SUCCESS run?  Returns True if: - Failure is NON_GATING (doesn't affect run success) - OR failure 

## app/arbormind/observation/failure_record.py

- `__post_init__(self)` (line 83): Validate on construction.
- `to_dict(self)` (line 107): Convert to dictionary for lightweight usage.

## app/arbormind/observation/failure_semantics.py

- `classify_failure(failure_id, error_message)` (line 128): Classify a failure into a semantic class.  PURE FUNCTION: No side effects, deterministic.  Args:     failure_id: The ID of the failure record     erro
- `_get_db_path()` (line 173): Get path to failure memory database.
- `_ensure_schema(conn)` (line 179): Ensure failure_semantics table exists.
- `record_semantic_classification(classification)` (line 185): Record a semantic classification (append-only).  NEVER raises - observability should not crash execution.
- `reclassify_all_failures(rules_version)` (line 221): Re-run classification on all existing failures.  This is safe because: - Append-only (new records, doesn't modify old) - Deterministic (same input → s
- `get_semantic_stats()` (line 278): Get count of failures by semantic class.
- `get_failures_by_class(semantic_class, limit)` (line 303): Get recent failures of a specific semantic class.

## app/arbormind/observation/interpretation_context.py

- `capture_current(cls)` (line 65): Capture current signal extractor state.
- `to_dict(self)` (line 90)
- `capture_current(cls)` (line 110): Capture current canon state.
- `to_dict(self)` (line 126)
- `capture_current(cls)` (line 146): Capture current scope semantics.
- `to_dict(self)` (line 165)
- `__post_init__(self)` (line 195)
- `capture_current(cls)` (line 211): Capture complete current interpretation context.
- `to_dict(self)` (line 221)
- `to_json(self)` (line 231): Serialize to JSON for storage.
- `from_dict(cls, d)` (line 236): Reconstruct from stored dict.
- `get_current_context()` (line 267): Get current interpretation context (cached per session).  Recomputed if underlying hashes change.
- `get_context_hash()` (line 285): Get just the context hash (lightweight check).
- `get_context_json()` (line 290): Get context as JSON string for storage.
- `verify_context_compatibility(stored_hash)` (line 299): Check if a stored context hash matches current context.  If False, the stored failure was recorded under different interpretation rules. This is infor
- `context_drift_warning(stored_hash)` (line 309): Generate a warning message if context has drifted.  Returns None if contexts match, warning message otherwise.

## app/arbormind/observation/observer.py

- `__init__(self, ledger)` (line 33): Initialize observer. ledger arg is legacy, now uses SQLite.
- `record(self)` (line 37): Record a branch execution event.  Writes to both JSONL ledger (legacy) and SQLite (new).
- `record_step_decision(self, run_id, step, agent, action, ...)` (line 86): Record a step-level decision (proto-V vector).  This is the KEY method for building learning data.
- `record_step_failure(self, run_id, step, failure_type, message, ...)` (line 114): Record a step failure (learning gold).  failure_type: invariant | truncation | quality | timeout | exception
- `record_verdict(self, run_id, step, agent, verdict, ...)` (line 139): Record a supervisor verdict (preference shaping).  verdict: approved | rejected
- `get_observer()` (line 173): Get singleton observer instance.
- `record_event(branch, decision)` (line 185): Phase 9: Observation-only logging.  HARD GUARANTEES: - Never blocks execution - Never mutates state - Exceptions swallowed
- `observe_step_start(run_id, step, agent)` (line 214): Record start of a step execution.
- `observe_step_success(run_id, step, agent, files_count)` (line 225): Record successful step completion.
- `observe_step_failure(run_id, step, agent, failure_type, message, ...)` (line 236): Record step failure with categorization.

## app/arbormind/observation/ontology_classifier.py

- `extract_tool_from_error(raw_error)` (line 224): Extract tool name from error message.  Looks for patterns like: - Tool 'syntaxvalidator' failed - [syntaxvalidator] Error: - syntaxvalidator: failed
- `detect_temporal_position(artifacts_before, artifacts_after)` (line 256): Determine when failure occurred relative to artifact materialization.  PURE FUNCTION: No side effects.
- `detect_repeatability(raw_error)` (line 277): Determine repeatability signature from error message.  PURE FUNCTION: Pattern matching only.
- `detect_artifact_impact(execution_layer, gating_semantics, artifacts_created)` (line 305): Determine artifact impact from other axes and artifact count.  PURE FUNCTION: No inference beyond explicit rules.
- `recommend_fclass(original_fclass, execution_layer, gating_semantics)` (line 341): Recommend F-class based on ontology classification.  This may reclassify F7 → F10 for observation layer failures. The original class is preserved; thi
- `classify_failure_ontology(tool_name, step_name, agent, primary_class, raw_error, ...)` (line 366): Deterministically classify a failure across all 7 axes.  PURE FUNCTION: - No side effects - Same input → same output (always) - No LLM - No heuristics

## app/arbormind/observation/ontology_migration.py

- `_get_failure_memory_db()` (line 47): Get path to failure_memory.db.
- `check_schema_v3_exists(conn)` (line 94): Check if schema v3 columns already exist.
- `apply_schema_v3(conn)` (line 101): Apply schema v3 migration (adds 7-axis columns).  Returns True if migration was applied, False if already applied.
- `backfill_ontology_classification(force_reclassify, dry_run)` (line 143): Re-classify all existing failures with ontology axes.  SAFE because: - Updates existing records (doesn't delete) - Tracks ontology_version for audit -
- `get_ontology_stats()` (line 281): Get distribution statistics for ontology axes.  Returns counts for each axis value.
- `get_non_gating_failures(limit)` (line 374): Get failures that are NON_GATING (telemetry only).  These are the failures that explain SUCCESS runs with HARD failures.
- `get_true_f7_failures(limit)` (line 403): Get TRUE F7 failures (after ontology reclassification).  Excludes observation layer failures that were incorrectly classified as F7.
- `main()` (line 438): CLI entry point for ontology migration.

## app/arbormind/observation/run_event.py

- `from_decision(cls, branch, decision)` (line 40): Create a RunEvent from a branch and execution decision.  Args:     branch: Branch object being observed     decision: ExecutionDecision from router   

## app/arbormind/observation/signal_extractor.py

- `__hash__(self)` (line 85)
- `to_dict(self)` (line 88): Convert to JSON-serializable dict.
- `from_dict(cls, d)` (line 97): Reconstruct from dict.
- `unique_signals(self)` (line 120): Get deduplicated set of signals.
- `signal_count(self)` (line 125): Total signals extracted (including duplicates).
- `unique_count(self)` (line 130): Unique signals extracted.
- `to_list(self)` (line 134): Convert all signals to JSON-serializable list.
- `extract_from_error(self, error_text)` (line 190): Extract signals from an error message.  Pure function: same input → same output.
- `extract_from_diff(self, diff_text)` (line 273): Extract signals from a file diff.  Only extracts added (+) and removed (-) lines. No interpretation of what changed.
- `extract_error_tokens(self, error_text, max_tokens)` (line 303): Tokenize error message into countable tokens.  Rules: - Lowercase - Split on whitespace and punctuation - Remove pure numbers - Remove very short toke
- `extract_signals(error_text)` (line 348): Convenience function for error signal extraction.
- `extract_diff_signals(diff_text)` (line 353): Convenience function for diff signal extraction.
- `tokenize_error(error_text)` (line 358): Convenience function for error tokenization.

## app/arbormind/observation/step_state_snapshot.py

- `compute_paths_hash(paths)` (line 66): Hash of sorted artifact paths.
- `compute_workspace_hash(manifest)` (line 73): Hash of workspace state (paths + sizes).
- `capture_workspace_state(project_path)` (line 81): Capture current workspace artifact inventory.  Returns:     (manifest, artifact_count)
- `create_snapshot(run_id, step_name, position, project_path)` (line 124): Create a step state snapshot.  Pure function - no side effects, no database writes.
- `record_step_state_snapshot(snapshot)` (line 154): Record a step state snapshot to database.  MUST NEVER CRASH EXECUTION. Fire-and-forget.  Returns True if recorded, False otherwise (but never raises).
- `record_step_entry(run_id, step_name, project_path)` (line 195): Record workspace state at step entry.  Call this BEFORE step execution begins.
- `record_step_exit(run_id, step_name, project_path)` (line 213): Record workspace state at step exit.  Call this AFTER step execution completes (success or failure).
- `get_step_snapshots(run_id, step_name)` (line 235): Get snapshots for a run (optionally filtered by step).  For offline analysis only - NOT during execution.
- `get_step_state_diff(run_id, step_name)` (line 268): Get entry and exit state for a step.  Returns {entry: ..., exit: ..., changed: bool}  For offline analysis only. NOTE: This does NOT interpret WHY it 

## app/arbormind/observation/tool_trace.py

- `truncate_payload(obj)` (line 158): Truncate payload for forensic storage.  Rules: 1. JSON only 2. UTF-8 only   3. Max 2048 bytes total 4. Max 512 chars per value 5. No binary 6. No secr
- `truncate_error(error)` (line 211): Truncate error message to reasonable length.
- `_get_db_path()` (line 226): Get path to TIT database (same as arbormind.db).
- `_ensure_schema(conn)` (line 232): Ensure TIT table exists.
- `_get_connection()` (line 251): Get database connection.
- `record_tool_invocation(event)` (line 263): Fire-and-forget tool invocation recording.  MUST NEVER RAISE.  This is the ONLY function that writes to tool_invocations. All other paths are forbidde
- `build_tool_event(run_id, step, agent, tool_name, tool_type, ...)` (line 326): Build a ToolInvocationEvent with proper truncation.  Convenience function for hook points.

## app/arbormind/priors/tool_priors.py

- `__init__(self, weights)` (line 24): Initialize with tool weights.  Args:     weights: Dict mapping tool name to prior weight
- `bias(self, tool_name)` (line 36): Get the prior bias for a tool.
- `_clamp(cls, value)` (line 41): Clamp a prior weight to safe bounds.
- `from_success_rates(cls, success_rates)` (line 46): Create priors from historical success rates.
- `get_tool_prior(tool_name)` (line 60): Convenience helper for the ExecutionRouter.  Currently returns neutral prior (1.0). In the future, this will load from the learned ToolPriors singleto

## app/arbormind/reconstruction/run_slice_builder.py

- `__init__(self)` (line 32)
- `build_slice(self, run_id)` (line 35): Reconstruct the full run state from raw events.
- `_reassemble_steps(self, events)` (line 64): Convert entry/exit event stream into duration objects.
- `get_run_slice(run_id)` (line 84)

## app/arbormind/runtime/actions.py

- `get_action_for_step(step_name)` (line 80): Get the action type for a step.
- `get_steps_for_action(action)` (line 85): Get all steps that can fulfill an action.
- `select_next_action(branch, completed_steps, failed_steps)` (line 90): Select the next action based on branch state.  This is the COGNITIVE PIVOT: we choose what to do, not just march forward.
- `map_action_to_step(action, branch, completed_steps)` (line 134): Map an action to a specific step based on what's been done.  Returns the next step to execute for this action.

## app/arbormind/runtime/execution_router.py

- `decide(self, branch, context)` (line 30): Decide the next execution action for a branch.

## app/arbormind/runtime/observer.py

- `observe(branch, report)` (line 5)

## app/arbormind/runtime/runtime.py

- `__init__(self)` (line 28)
- `cycle(self, tree)` (line 31)
- `record_observations(self, tree)` (line 83): Record observations AFTER execution completes.  INVARIANT: This runs AFTER all cognitive decisions. INVARIANT: Observer failures MUST NOT halt executi

## app/arbormind/storage/run_ledger.py

- `append_event(event, ledger_path)` (line 24): Append a single event to the ledger.  Each event is written as a complete JSON line (JSONL format). This is crash-safe: partial writes only affect the
- `__init__(self, path)` (line 71): Initialize ledger at the given path.  Args:     path: Path to JSONL file (will be created if it doesn't exist)
- `append(self, event)` (line 81): Append a single event to the ledger.  Delegates to module-level append_event function.

## app/arbormind/watch_ledger.py

- `clear_screen()` (line 12)
- `get_connection()` (line 15)
- `fetch_latest_run(conn)` (line 24)
- `fetch_run_steps(conn, run_id)` (line 32): Reconstruct steps from events for display.
- `fetch_recent_failures(conn, run_id)` (line 60)
- `fetch_recent_decisions(conn, run_id)` (line 68)
- `main()` (line 76)

## app/core/auth_boundary.py

- `is_system_entity(entity_name)` (line 46): Check if an entity is a system entity (User, Auth, etc).  System entities should NOT have domain CRUD routers.
- `validate_entity_list(entities)` (line 56): Validate that entity list doesn't violate auth boundary.  Returns:     {         "valid": bool,         "issues": List[str],         "filtered_entitie
- `check_router_code_for_violations(router_code, entity_name)` (line 83): Check router code for auth boundary violations.  Returns:     None if valid, violation message if invalid
- `get_auth_guidance()` (line 110): Get guidance text for proper auth implementation.  This should be injected into prompts to prevent auth violations.
- `should_skip_entity_for_router(entity_name)` (line 168): Determine if an entity should be skipped for router generation.  Returns True for system entities (User, Auth, etc).

## app/core/config.py

- `ensure_directories(self)` (line 114): Ensure required directories exist.

## app/core/domain_grounding.py

- `infer_entities_from_archetype(archetype)` (line 82): Infer minimum viable entities from archetype.  Returns:     Set of entity names (capitalized)
- `infer_entities_from_user_request(user_request)` (line 104): Extract potential entities from user request using keywords.  This is a simple heuristic, not NLP.
- `validate_domain_entities(detected_entities, archetype, user_request)` (line 144): Validate that project has meaningful domain entities.  Returns:     {         "valid": bool,         "issues": List[str],         "suggested_entities"
- `apply_entity_grounding(entities, archetype, user_request)` (line 194): Apply domain entity grounding to ensure meaningful entities.  If User is the only entity, inject archetype-appropriate entities.  Returns:     List of

## app/core/exceptions.py

- `__init__(self, message, details)` (line 10)
- `__init__(self, step, quality_score, threshold)` (line 23)
- `__init__(self, agent_name, message, attempt)` (line 35)
- `__init__(self, provider, message)` (line 46)
- `__init__(self, provider, retries)` (line 56)
- `__init__(self, project_id, message)` (line 67)
- `__init__(self, path, message)` (line 77)

## app/core/execution_record.py

- `to_dict(self)` (line 28): Serialize for checkpoint persistence.
- `from_dict(cls, data)` (line 38): Deserialize from checkpoint.

## app/core/failure_boundary.py

- `enforce(fn)` (line 34): Decorator that catches all failures and classifies them.  Prevents legacy paths from bypassing taxonomy.  Usage:     @FailureBoundary.enforce     asyn
- `async wrapper()` (line 46)
- `convert(legacy_result)` (line 157): Convert legacy StepResult to new StepExecutionResult.  Args:     legacy_result: Old-style StepResult object      Returns:     New StepExecutionResult 

## app/core/failure_reporting.py

- `report_failure(step_name, failure_type, reason, details, is_causal, ...)` (line 56): Report a failure honestly and clearly.  This is the SINGLE SOURCE OF TRUTH for failure reporting.  Args:     step_name: The workflow step that failed 
- `report_truncation(step_name, file_path, expected_sentinel)` (line 109): Report a truncation failure (output cut off).  This is a CHEAP check that saves thousands of tokens.
- `report_malformed_output(step_name, parse_error)` (line 132): Report a malformed output failure (JSON parse failed, etc.)
- `report_validation_failure(step_name, validation_errors)` (line 151): Report a validation failure (syntax error, empty file, etc.)
- `generate_failure_summary(failures)` (line 174): Generate a human-readable failure summary.  This is the message shown to the user when the workflow fails.

## app/core/file_writer.py

- `convert_files_list_to_dict(files)` (line 11): Convert list of {"path": ..., "content": ...} to dict {path: content}.
- `async write_validated_files(project_path, files, step)` (line 18): Validate and write LLM-generated files to disk.  Args:     project_path: Base project directory     files: List of {"path": ..., "content": ...} dicts
- `async persist_agent_output(manager, project_id, project_path, parsed, step, ...)` (line 82): Legacy wrapper for write_validated_files. Extracts files from parsed dict and writes them.
- `validate_file_output(parsed, step, max_files)` (line 97): Legacy wrapper - validates and returns the same parsed dict.
- `async safe_write_llm_files(manager, project_id, project_path, files, step_name, ...)` (line 117): Legacy compatibility wrapper for write_validated_files.

## app/core/guard.py

- `__getattr__(self, name)` (line 13)

## app/core/integration.py

- `__init__(self, project_id)` (line 55): Initialize integration manager for a project
- `_load_config(self)` (line 60): Load configuration from environment and defaults
- `is_feature_enabled(self, feature)` (line 73): Check if a specific feature is enabled.
- `get_memory_db_path(self)` (line 87)
- `get_max_workflow_turns(self)` (line 90)
- `get_max_test_retries(self)` (line 93)
- `validate_api_keys(self)` (line 96): Validate that required API keys are configured.
- `get_config_summary(self)` (line 117): Get summary of current configuration.
- `create_integration_manager(project_id)` (line 139)
- `validate_environment()` (line 143): Validate that the environment is properly configured.

## app/core/llm_output_integrity.py

- `_is_truncated(content)` (line 13)
- `_validate_python(content, path)` (line 24)
- `_validate_js_like(content, path)` (line 33)
- `validate_llm_files(files, step)` (line 43): Hygiene-only validation. Ensures files are readable, complete, and syntactically sane.  RETURNS: same files dict if valid RAISES: LLMOutputIntegrityEr

## app/core/logging.py

- `log(scope, message, data, project_id)` (line 40): Unified logging function for GenCode Studio.  PHASE E4: Only INFO_SCOPES are shown by default. Set GENCODE_DEBUG=true to see all scopes.
- `log_section(scope, title, project_id)` (line 66): Log a section header with visual separator.
- `log_thinking(scope, thinking, project_id, max_lines)` (line 80): Log agent thinking/reasoning with proper formatting.
- `log_files(scope, files, project_id)` (line 98): Log file list summary.
- `log_result(scope, approved, quality, issues, project_id, ...)` (line 106): Log review result with quality score.

## app/core/step_invariants.py

- `require_files(parsed, step_name, min_files, required_patterns)` (line 31): INVARIANT: Step must produce at least min_files files.  Args:     parsed: The parsed LLM output containing "files" list     step_name: Name of current
- `require_complete(parsed, step_name)` (line 72): INVARIANT: Output must be complete (HDAP - all files have END_FILE).  Args:     parsed: The parsed HDAP output with "complete" and "incomplete_files" 
- `require_non_empty_content(parsed, step_name)` (line 100): INVARIANT: All files must have non-empty content.  Raises:     StepInvariantError if any file has empty content
- `require_approval_with_files(result, parsed, step_name)` (line 124): INVARIANT: Approval is only valid if files were actually produced.  This prevents the "approved with 0 files" bug.  Raises:     StepInvariantError if 
- `require_router_files(project_path, min_routers)` (line 148): INVARIANT: Project must have at least min_routers router files.  Used by system_integration to verify backend is ready.  Returns:     List of router f
- `require_architecture(project_path)` (line 183): INVARIANT: architecture.md must exist.  Returns:     Contents of architecture.md      Raises:     StepInvariantError if missing
- `require_testids(parsed, step_name, required_ids, primary_entity)` (line 208): INVARIANT: JSX files must contain required data-testid attributes.  This is a PREFLIGHT CHECK that runs BEFORE Marcus review. Missing testids are caug
- `validate_step_output(result, parsed, step_name, min_files)` (line 273): One-call validation for most step handlers.  Checks: 1. HDAP completeness (no truncated files) 2. Minimum file count 3. No empty files 4. Approval onl

## app/core/step_outcome.py

- `is_successful(self)` (line 34): Only SUCCESS counts as success.
- `requires_healing(self)` (line 38): Only COGNITIVE_FAILURE triggers healing.
- `is_dead_branch(self)` (line 42): Isolated steps are dead branches.
- `is_hard_failure(self)` (line 46): HARD_FAILURE is a global truth (ignores isolation).

## app/core/types.py

- `to_dict(self)` (line 26)
- `to_dict(self)` (line 39)
- `total(self)` (line 81)
- `estimated_cost(self)` (line 85): Estimate cost based on Gemini pricing.

## app/db/__init__.py

- `async connect_db()` (line 14): Connect to MongoDB.  If MongoDB is not available, stores the error for later retrieval rather than silently failing.
- `async disconnect_db()` (line 60): Disconnect from MongoDB.
- `get_db()` (line 68): Get database instance.  Returns None if MongoDB is not connected. Check is_connected() before assuming database operations will work.
- `is_connected()` (line 78): Check if database is connected.
- `get_connection_error()` (line 83): Get connection error message if connection failed.
- `get_collection(name)` (line 88): Get a collection by name.  Returns None if database is not connected. Caller should check is_connected() first or handle None case.

## app/handlers/architecture.py

- `async step_architecture(branch)` (line 27): Step 1: Victoria creates architecture plan with Marcus supervision.  Produces: - architecture.md with system design - Component hierarchy - Data flow 

## app/handlers/backend_models.py

- `async step_backend_models(branch)` (line 25): Step: Backend Models - Generate all models at once.  Two-phase approach: 1. Derek generates model specifications 2. System merges into single models.p
- `async derek_generate_model_spec(entities, relationships, contracts, project_id, manager, ...)` (line 211): Derek generates model specifications using supervised agent call.  Now uses ARTIFACT enforcement + auto-recovery for consistency.  Args:     entities:
- `_synthesize_entity_plan(project_path, branch)` (line 450): Synthesize entity_plan.json from detected entities when it doesn't exist.  This enables the unified two-step flow for ALL projects, even if the contra

## app/handlers/backend_routers.py

- `async step_backend_routers(branch)` (line 25): Step 4: Backend Router Implementation.  Flow: 1. Load entity plan and models.py 2. For each AGGREGATE entity, generate its FastAPI router
- `_build_single_router_prompt(entity, architecture_backend)` (line 242): Build prompt for generating a single entity's router.
- `_extract_entity_contract(contracts, entity_plural)` (line 460): Extract the entity-specific section from architecture.md.  Finds headings like "### Expenses" or "## Expenses" and extracts everything until the next 

## app/handlers/base.py

- `_get_broadcast_to_project()` (line 12): Lazy import to avoid circular dependency with app.workflow package.
- `async broadcast_status(manager, project_id, step, status, step_number, ...)` (line 21)
- `async broadcast_agent_log(manager, project_id, scope, message, data, ...)` (line 47): Broadcast an agent log/thinking message.

## app/handlers/frontend_mock.py

- `async step_frontend_mock(branch)` (line 45): Step 2: Derek generates frontend with MOCK DATA first.  This creates the immediate "aha moment" for users - they see a working UI before any backend i

## app/handlers/preview.py

- `async step_preview_final(branch)` (line 26): Step 11: Refresh preview on RANDOM free ports for both frontend and backend.

## app/handlers/refine.py

- `async step_refine(branch)` (line 33): Refine Mode - Conversational Iteration (post-workflow).  Allows the user to modify the existing codebase using natural language. Example: "Change the 

## app/handlers/system_integration.py

- `async step_system_integration(branch)` (line 16): Step 5: System Integration (Pure Python).  Deterministic wiring of ALL generated modules: - Backend: Wire routers and models into main.py - Frontend: 
- `_generate_api_helpers(project_path, routers)` (line 120): Generate frontend/src/lib/api.js with API helper functions.
- `_replace_mock_with_api(page_file, routers)` (line 179): Replace mock data imports with API calls in a JSX page.
- `_singularize(plural)` (line 214): Simple pluralization remover for common cases.

## app/handlers/testing_backend.py

- `render_contract_tests(template_path, contracts_md, output_path, entity_name, entity_plural, ...)` (line 44): Render the template deterministically. Source of truth: architecture.md (used to verify scope, implemented via template)  Rules: NO Derek NO healing N
- `async _generate_tests_from_template(manager, project_id, project_path, user_request, primary_entity, ...)` (line 78): Generate backend tests from template at the START of testing step.  Flow: 1. Read the test template (from Golden Seed) 2. Call Derek to generate proje
- `async step_testing_backend(branch)` (line 299): Derek tests backend using sandbox.  ONE SHOT POLICY: - Executes ONCE per attempt. - No internal retry loops. - No internal self-healing. - If it fails

## app/handlers/testing_frontend.py

- `ensure_str(val)` (line 45): Ensure value is a string (sandboxexec may return bytes).
- `async _generate_frontend_tests_from_template(manager, project_id, project_path, user_request, primary_entity, ...)` (line 54): Generate frontend E2E tests from template at the START of testing step.  Flow: 1. Read the test template (from Golden Seed) 2. Call Luna to generate p
- `async step_testing_frontend(branch)` (line 239): Step 11: Luna tests frontend with Playwright.  Execution-Only Pattern: - Tests are generated and executed in a single-shot. - No internal healing or i
- `persist_test_file_for_debugging(attempt_num, test_content, source)` (line 286): Save test file content for debugging purposes.

## app/lib/monitoring.py

- `set_active_preview_servers(n)` (line 18): Sets the value of the active preview servers gauge.
- `register_monitoring(app)` (line 22): Registers Prometheus monitoring on the FastAPI app. This replaces the Express middleware and /metrics endpoint.

## app/lib/secrets.py

- `encrypt_secret(plaintext, key_base64)` (line 22): Encrypts a plaintext secret using AES-256-GCM.  Args:     plaintext: The secret to encrypt     key_base64: Base64-encoded 32-byte key for AES-256
- `decrypt_secret(encrypted_secret, key_base64)` (line 59): Decrypts an AES-256-GCM encrypted secret.
- `async fetch_vault_secret(vault_url, token, secret_path)` (line 92): Fetches a single secret from Vault using its v1 kv API.
- `async load_secrets_from_vault(mapping)` (line 116): Load a mapping of env names to Vault secret paths and set os.environ values when found.

## app/lib/websocket.py

- `__init__(self)` (line 15)
- `async connect(self, websocket, project_id)` (line 21)
- `async disconnect(self, websocket, project_id)` (line 28): Thread-safe disconnect.
- `async send_json(self, websocket, data)` (line 37)
- `async send_to_project(self, project_id, message)` (line 40): Send a JSON message to all clients connected for a given project_id. Thread-safe: takes a snapshot of connections under lock.
- `async broadcast_json(self, message)` (line 62): Broadcast a JSON message to all connected websockets across all projects.
- `async broadcast(self, project_id, message)` (line 70): Convenience / backwards-compatible alias: broadcast to a single project.

## app/llm/adapter.py

- `get_stop_sequences(file_type)` (line 36): Get appropriate stop sequences for a file type.
- `get_stop_sequences_for_step(step_name)` (line 41): Get appropriate stop sequences based on workflow step.  Prevents Derek from generating incomplete functions by stopping at natural code boundaries lik
- `__init__(self)` (line 85)
- `async call(self, prompt, system_prompt, provider, model, ...)` (line 89): Call an LLM provider with automatic retry.  Args:     prompt: The user/assistant prompt     system_prompt: System instructions     provider: Provider 
- `async _call_provider(self, provider, model, prompt, system_prompt, ...)` (line 150): Call a specific provider - SINGLE ATTEMPT ONLY.  ArborMind handles retry decisions via branch continuation. This adapter is pure execution muscle.
- `async call_llm(prompt, system_prompt, provider, model, temperature, ...)` (line 204): Convenience function for calling LLM with V2 stop sequences support.
- `async call_llm_with_usage(prompt, system_prompt, provider, model, temperature, ...)` (line 228): V3: Call LLM and return BOTH text and usage metadata.  Returns:     Dict with {"text": str, "usage": {"input": int, "output": int}}

## app/llm/artifact_enforcement.py

- `enforce_artifact_mode(base_system_prompt, user_task, step_name, files, contracts, ...)` (line 63): Enforce ARTIFACT mode by building prompts correctly.  Returns:     {         "system_prompt": "...",  # Protocol rules + agent identity         "user_
- `async auto_recover_hdap(raw_output, agent_name, llm_call_func, provider, model, ...)` (line 129): Deterministic HDAP recovery: Single automatic attempt to re-wrap output.  Rules: - Only called if HDAP markers are missing - Single attempt (no loops)

## app/llm/prompt_management.py

- `filter_files_for_step(step, files, max_files)` (line 26)
- `build_context()` (line 155): Build the final prompt sent to the LLM.  ⚠️ CRITICAL GUARANTEE: - The LLM NEVER sees file metadata, schemas, path/content structures,   or any represe

## app/llm/providers/anthropic.py

- `async call(prompt, system_prompt, model, temperature, max_tokens, ...)` (line 14): Call Anthropic Claude API.  Args:     stop_sequences: V2 - sequences that signal completion (prevents truncation)  Returns:     The generated text    

## app/llm/providers/gemini.py

- `async call(prompt, system_prompt, model, temperature, max_tokens, ...)` (line 14): Call Google Gemini API.  Args:     stop_sequences: V2 - sequences that signal completion (prevents truncation)  Returns:     The generated text      R

## app/llm/providers/ollama.py

- `async call(prompt, system_prompt, model, temperature, max_tokens, ...)` (line 13): Call Ollama API (local).  Args:     stop_sequences: V2 - sequences that signal completion (prevents truncation)  Returns:     The generated text      

## app/llm/providers/openai.py

- `async call(prompt, system_prompt, model, temperature, max_tokens, ...)` (line 14): Call OpenAI API.  Args:     stop_sequences: V2 - sequences that signal completion (prevents truncation)  Returns:     The generated text      Raises: 

## app/main.py

- `async lifespan(app)` (line 64): Application startup and shutdown.
- `async websocket_endpoint(websocket, project_id)` (line 155)
- `async serve_spa(request, full_path)` (line 212)

## app/orchestration/backend_probe.py

- `async check_route(self, method, path, expected_status, timeout, ...)` (line 29): Test if a route returns the expected status code.  Args:     method: HTTP method (GET, POST, etc.)     path: Route path (e.g., /api/health)     expect
- `async is_healthy(self, timeout)` (line 51): Check if backend is healthy via /api/health.
- `from_env(cls, mode)` (line 56): Factory method to create probe based on environment.
- `__init__(self, mode, project_id)` (line 69): Initialize HTTP probe.  Args:     mode: ProbeMode.DOCKER or ProbeMode.SANDBOX     project_id: Project ID for dynamic port detection
- `async _get_base_url(self)` (line 85): Get the backend base URL dynamically (port detection for Docker).
- `async check_route_detailed(self, method, path, expected_status, timeout, ...)` (line 112): Test route via HTTP request and return detailed status.
- `async check_route(self, method, path, expected_status, timeout, ...)` (line 171): Wrapper for backward compatibility.
- `async is_healthy(self, timeout)` (line 182): Check health endpoint.
- `__init__(self, project_path)` (line 190): Initialize ASGI probe.  Args:     project_path: Path to project workspace
- `_get_app(self)` (line 200): Lazy-load FastAPI app.
- `async check_route(self, method, path, expected_status, timeout, ...)` (line 212): Test route via ASGI transport.
- `async is_healthy(self, timeout)` (line 243): Check health endpoint.

## app/orchestration/budget_manager.py

- `__init__(self, config)` (line 134)
- `_reset_run_state(self)` (line 145): Reset state for a new run.
- `start_run(self)` (line 153): Reset budget usage for a new FAST run.
- `max_usd(self)` (line 163): Maximum budget in USD.
- `_estimate_cost_usd(self, input_tokens, output_tokens)` (line 167): Estimate cost in USD for given token counts.
- `_remaining_usd(self)` (line 174): Remaining budget in USD.
- `remaining_inr(self)` (line 179): Remaining budget in INR.
- `used_inr(self)` (line 184): Used budget in INR.
- `get_step_policy(self, step)` (line 191): Get policy for a step (with sensible defaults).
- `allowed_attempts_for_step(self, step)` (line 198): Decide how many attempts this step is allowed, given the remaining budget.  Returns a number in [0, policy.max_attempts]. 0 means "no attempts allowed
- `can_afford_step(self, step)` (line 237): Quick check if we can afford at least one attempt of this step.
- `register_usage(self, input_tokens, output_tokens, step, agent, ...)` (line 244): Call this after each ACTUAL Gemini call, using the real 'usage' object from the API response.
- `get_budget_status(self)` (line 287): Return human-readable budget status emoji.
- `get_usage_summary(self)` (line 300): Get complete usage summary for logging/debugging.
- `log_status(self, prefix)` (line 317): Print current budget status to console.
- `get_budget_manager(project_id)` (line 336): Get or create a BudgetManager instance.  If project_id is provided, returns a per-project instance. Otherwise returns a shared default instance.
- `reset_budget_manager(project_id)` (line 357): Reset a BudgetManager instance.
- `get_all_project_budgets()` (line 370): Get all tracked project budgets.
- `get_budget_for_api(project_id)` (line 376): Get budget data formatted for the frontend CostDashboard API.  Returns data in the format expected by CostDashboard.tsx: - total_input_tokens, total_o

## app/orchestration/checkpoint.py

- `__init__(self, base_dir)` (line 23)
- `async save_project_snapshot(self, project_path, step)` (line 27): Capture the entire project state from disk. Async wrapper for blocking IO.
- `_save_project_snapshot_sync(self, project_path, step)` (line 34): Sync implementation of project capture.
- `save(self, step, files)` (line 52): Save a checkpoint for a step using provided file content.
- `get_latest(self, step)` (line 79): Get the latest checkpoint for a step.
- `list_checkpoints(self, step)` (line 117): List all checkpoints, optionally filtered by step.

## app/orchestration/context.py

- `get_relevant_files(project_path, step_name, max_content_size)` (line 66): Get only the files relevant to the current step. Returns a formatted string of file contents.
- `get_entity_context(project_id, user_request, project_path)` (line 131): Get entity-specific context for agents. Tells them what the primary entity is and where to find/create related files.
- `get_previous_files_summary(project_path, max_files)` (line 195): Get a brief summary of what files exist in the project. Useful for agents to understand what's already been created.
- `__init__(self, project_id)` (line 245)
- `_ctx(self)` (line 256)
- `record_step_completion(self, step, summary)` (line 259): Record that a step completed with optional summary data.
- `set_entities(self, entities)` (line 266): Set the primary entities from architecture step.
- `set_architecture(self, arch_summary)` (line 271): Set architecture summary from architecture step.
- `get_summary_for_step(self, current_step)` (line 275): Get a context summary tailored for the current step.  Returns a formatted string with relevant context from previous steps.
- `get_completed_steps(self)` (line 303): Get list of completed steps.
- `reset(self)` (line 307): Reset context for a new workflow run.
- `get_or_create(cls, project_id)` (line 317): Factory method to get or create context for a project.

## app/orchestration/fast_orchestrator.py

- `__init__(self, project_id, manager, project_path, user_request, ...)` (line 101)
- `_register_usage(self, step, result)` (line 147): Extract and register token usage from mixed result types.
- `async run(self)` (line 169): Execute the branch. Minimal logic, only execution.
- `_validate_step_output(self, step)` (line 778): Minimal output validation gate.
- `async _save_checkpoint(self, step)` (line 784): Save a checkpoint after successful step completion.
- `async _record_step_context(self, step, result)` (line 806): Record step completion to cross-step context.
- `_load_execution_state(self)` (line 856): Phase-0: Rebuild internal state from disk checkpoints. Populates self.completed_steps and self.execution_records.
- `_register_step_files_from_paths(self, step_name, file_paths)` (line 910): Phase-0: Register files created by a step using raw paths.
- `_get_project_snapshot(self)` (line 919): Capture a snapshot of the project filesystem for change detection.
- `_compute_file_delta(self, before, after)` (line 947): Identify files that were created or modified between snapshots.
- `_register_step_files(self, step_name, files)` (line 956): Legacy: Register files from dict list.
- `_rollback_step(self, step_name)` (line 963): Phase-0: Delete files created by a failed step. Best-effort rollback - continues even if deletions fail.
- `async _retry_step_with_hardened_prompt(self, step_name, handler, branch)` (line 989): Phase-1: Retry a failed step with hardened prompt. Returns (success: bool, result: Any)
- `_generate_halt_artifact(self, step_name, reason, attempts)` (line 1029): Phase-1: Generate halt artifact for handoff to Phase-2.
- `_should_retry_step(self, step_name)` (line 1054): Phase-1: Determine if a step should be retried.
- `async run_fast_v2_workflow(project_id, manager, project_path, user_request, provider, ...)` (line 1065)

## app/orchestration/file_persistence.py

- `__init__(self, base_path)` (line 21)
- `write(self, path, text)` (line 24): Atomically write text to a file. Uses a temporary file and rename to prevent partial writes.  Returns:     True if successful, False otherwise
- `read(self, path)` (line 80): Read a file and return its contents, or None if not found.
- `exists(self, path)` (line 94): Check if a file exists.
- `delete(self, path)` (line 99): Delete a file if it exists.
- `set_base_path(self, base_path)` (line 113): Set the base path for all operations.
- `ensure_dir(self, path)` (line 117): Ensure a directory exists.

## app/orchestration/llm_output_integrity.py

- `_is_truncated(content)` (line 13)
- `_validate_python(content, path)` (line 24)
- `_validate_js_like(content, path)` (line 33)
- `validate_llm_files(files, step)` (line 43): Hygiene-only validation. Ensures files are readable, complete, and syntactically sane.  RETURNS: same files dict if valid RAISES: LLMOutputIntegrityEr

## app/orchestration/router_utils.py

- `is_router_imported(content, router_name)` (line 15): Check if a router is already imported in main.py.  Handles all import variants: - from app.routers import notes - from app.routers import notes, auth,
- `is_router_registered(content, router_name)` (line 39): Check if a router is already registered in main.py.  Handles all registration variants: - app.include_router(notes.router, ...) - app.include_router(n
- `get_missing_routers(content, router_names)` (line 61): Get routers that are missing import or registration.  Args:     content: The main.py file content     router_names: List of router names to check     
- `get_routers_from_directory(routers_dir)` (line 77): Discover router names from a routers directory.  Args:     routers_dir: Path to the routers directory      Returns:     List of router names (without 

## app/orchestration/state.py

- `async get_session(project_id)` (line 23): Get or create session.
- `async is_running(project_id)` (line 32): Check if a workflow is running for a project.
- `async set_running(project_id, running)` (line 38): Set the running state for a project.
- `async try_start_workflow(project_id)` (line 46): Atomically check if workflow can start and mark it as running. FIX STATE-001: Uses DB atomicity (mostly) and lock for safety.
- `async stop_workflow(project_id)` (line 62): Atomically mark workflow as stopped.
- `async is_paused(project_id)` (line 71): Check if a workflow is paused for a project.
- `async get_paused_state(project_id)` (line 77): Get the paused workflow state.
- `async pause_workflow(project_id, step, turn, chat_history, user_request, ...)` (line 83): Pause a workflow for user input. State is persisted to DB.
- `async resume_workflow(project_id)` (line 112): Resume a paused workflow, returning the saved state and cleaning up.
- `async set_intent(project_id, intent)` (line 127): Store analyzed intent for a project.
- `async get_intent(project_id)` (line 134): Get stored intent for a project.
- `async set_original_request(project_id, request)` (line 140): Store original user request.
- `async get_original_request(project_id)` (line 147): Get original user request.
- `set_manager(project_id, manager)` (line 153): Store connection manager (Memory only).
- `get_manager(project_id)` (line 158): Get connection manager.
- `async cleanup(project_id)` (line 163): Clean up state (stop running, unpause). Does NOT clear completed_steps for resume.
- `async save_completed_step(project_id, step, context)` (line 179): Save a completed step to MongoDB for resume support.  Args:     project_id: Project identifier     step: Step name that completed     context: Optiona
- `async get_completed_steps(project_id)` (line 198): Get list of completed steps for a project.
- `async get_step_context(project_id)` (line 204): Get saved step context for resume.
- `async get_current_step(project_id)` (line 210): Get last completed step.
- `async clear_progress(project_id)` (line 216): Clear all progress for a fresh start.
- `async has_progress(project_id)` (line 227): Check if project has any saved progress.
- `async acquire_lock()` (line 233)
- `release_lock()` (line 237)
- `async set_architecture_cache(project_id, architecture_files)` (line 245): Cache architecture files generated by Victoria. Called after architecture step completes.  Args:     architecture_files: Dict like {"frontend": "...",
- `async get_architecture_cache(project_id)` (line 259): Get cached architecture files from Victoria's output. Falls back to empty dict if not cached yet.  Returns:     Dict like {"frontend": "...", "backend

## app/orchestration/structural_compiler.py

- `validate(cls, files)` (line 32)
- `_validate_directories(cls, paths)` (line 41)
- `_validate_required_files(cls, paths)` (line 49)
- `_validate_backend_routes(cls, paths)` (line 57)
- `_validate_frontend_presence(cls, paths)` (line 70)

## app/orchestration/task_graph.py

- `__init__(self)` (line 21)
- `get_steps(self)` (line 51): Get the ordered list of all workflow steps.
- `required_for(self, step)` (line 55): Get the dependencies required for a step.
- `is_ready(self, step, completed)` (line 59): Check if all dependencies for a step are satisfied.
- `get_blocking(self, step, completed)` (line 64): Get the dependencies that are NOT yet completed.
- `get_dependents(self, step)` (line 69): Get all steps that depend on this step.
- `get_parallel_batch(self, completed)` (line 77): Get all steps that can run in parallel right now.

## app/orchestration/token_policy.py

- `get_tokens_for_step(step_name, is_retry)` (line 140): Get appropriate token allocation for a workflow step.  Args:     step_name: Workflow step identifier (e.g., "backend_implementation")     is_retry: Wh
- `get_step_description(step_name)` (line 208): Get human-readable description of a workflow step.
- `get_all_policies()` (line 215): Get all token policies (for debugging/monitoring).
- `get_temperature(step_name, is_retry, failure_reason)` (line 252): Get appropriate temperature for a workflow step.  Args:     step_name: Workflow step identifier     is_retry: Whether this is a retry attempt     fail
- `get_retry_parameters(step_name, base_tokens, failure_reason)` (line 296): Get adjusted parameters for retry attempts.  Increases tokens and reduces temperature for better results.  Args:     step_name: Workflow step being re

## app/orchestration/utils.py

- `async broadcast_to_project(manager, project_id, message)` (line 11): Broadcast a message to all clients connected to a project.
- `pluralize(word)` (line 31): Simple English pluralization helper.  Rules: - Ends with 'y' (not preceded by vowel) -> replace 'y' with 'ies' - Ends with s, x, z, ch, sh -> add 'es'

## app/orchestration/wiring_utils.py

- `_safe_write_validated_python(path, content)` (line 25): Apply the SAME validation gate as LLM output: - Unicode normalization - AST validation
- `wire_router(project_path, router_name)` (line 49): Ensure router is wired in main.py (idempotent).
- `wire_model(project_path, model_name)` (line 133): Ensure model is imported AND registered in document_models list in main.py.  CRITICAL FOR BEANIE ODM.

## app/sandbox/__init__.py

- `get_sandbox()` (line 17): Get the sandbox singleton (lazy initialization).

## app/sandbox/health_monitor.py

- `__init__(self, docker_client)` (line 16)
- `async wait_for_healthy(self, project_id, containers, timeout)` (line 20): Wait for all containers to become healthy, based on Docker's healthcheck. Returns: { service_name: bool }
- `async _check_container_health(self, container_id, service_name, ports)` (line 60): Check if a specific container is healthy.  INVARIANT C: "Healthy" Means HTTP-Responsive (2025-12-17) A service is not healthy until it responds correc
- `async _check_http_health(self, ports)` (line 104): Perform actual HTTP health check to the backend /api/health endpoint. SINGLE ATTEMPT ONLY - ArborMind handles retry decisions.  Args:     ports: Docke
- `_inspect_container_state(self, container_id)` (line 142): Run `docker inspect` and return the .State dict, or None on failure.
- `async get_detailed_health(self, project_id, containers)` (line 167): Get detailed health information for all containers. Uses docker CLI only.  INVARIANT C: For backend, includes HTTP responsiveness status.

## app/sandbox/log_streamer.py

- `__init__(self, docker_client)` (line 17)
- `async _stream_logs_from_cli(self, stream_id, container_id, websocket_send)` (line 22): Internal helper: run `docker logs -f` on the container and forward each line to `websocket_send(line)`.  NOTE: On Windows, asyncio.create_subprocess_e
- `start_process()` (line 41)
- `async start_streaming(self, stream_id, container_id, websocket_send)` (line 75): Start streaming logs for a container.  `websocket_send` is an async function that takes a string log line.
- `async stop_streaming(self, stream_id)` (line 92): Stop a log stream, if running.
- `async stop_all_streams(self)` (line 107): Stop all active log streams.

## app/sandbox/pool.py

- `__init__(self, pool_size, warmup_timeout)` (line 47)
- `async initialize(self)` (line 57): Initialize the pool with pre-warmed containers.  Call this on application startup.
- `async acquire(self, project_id, project_path)` (line 87): Acquire a sandbox for a project.  Returns immediately if a pre-warmed container is available, otherwise falls back to on-demand creation.  Args:     p
- `async release(self, project_id)` (line 124): Release a sandbox back to the pool or stop it.  Args:     project_id: Project to release      Returns:     True if released successfully
- `async shutdown(self)` (line 155): Shutdown the pool and all containers.
- `stats(self)` (line 171): Get pool statistics.
- `async _maintain_pool(self)` (line 181): Background task to maintain pool size.
- `async _refill_pool(self)` (line 193): Refill the pool to maintain target size.
- `async _warm_container(self)` (line 205): Create and warm a new container.
- `async _create_on_demand(self, project_id, project_path)` (line 242): Fallback: Create sandbox on-demand using existing SandboxManager.
- `async _is_healthy(self, sandbox)` (line 272): Check if a sandbox container is healthy.
- `async _stop_sandbox(self, sandbox)` (line 278): Stop a sandbox container.
- `_sandbox_to_dict(self, sandbox)` (line 286): Convert PooledSandbox to dict.
- `get_sandbox_pool()` (line 302): Get the global sandbox pool instance.
- `async initialize_pool()` (line 310): Initialize the sandbox pool on app startup.

## app/sandbox/preview_manager.py

- `__init__(self)` (line 12)
- `create_preview(self, project_id, port)` (line 15): Return Traefik preview URL (no tunnel needed)
- `stop_preview(self, project_id)` (line 37): Stop a preview (optional - Traefik continues routing)
- `list_previews(self)` (line 54): List all active preview URLs

## app/sandbox/sandbox_config.py

- `to_dict(self)` (line 40): Convert config to dictionary
- `from_dict(cls, data)` (line 60): Create config from dictionary

## app/sandbox/sandbox_manager.py

- `__init__(self)` (line 23)
- `async get_status(self, project_id)` (line 37)
- `async create_sandbox(self, project_id, project_path, config)` (line 50)
- `async start_sandbox(self, project_id, wait_healthy, services)` (line 91): Start Docker-based backend/frontend containers for a project. Firecracker is only used for running tests; sandbox runtime stays on Docker.
- `async stop_sandbox(self, project_id)` (line 174): Stop Docker containers for this sandbox.
- `async destroy_sandbox(self, project_id)` (line 206): Stop containers and remove sandbox metadata. Does NOT delete the project files on disk (workspace stays).
- `async run_backend_tests(self, project_id, timeout)` (line 228): Run backend tests using Docker Compose. Compatible with WSL2 (no Firecracker/KVM required).  Returns:   - success   - stdout   - stderr   - returncode
- `async run_frontend_tests(self, project_id, timeout)` (line 275): Run frontend tests (Playwright) using Docker Compose. Compatible with WSL2 (no Firecracker/KVM required).  Returns:   - success   - stdout   - stderr 
- `async _run_compose_command(self, project_path, command, timeout)` (line 325)
- `run_sync_cmd()` (line 335)
- `async _get_compose_logs(self, project_path)` (line 369)
- `_validate_project_structure(self, project_path)` (line 380): Ensure golden backend/frontend/docker files exist before we start containers or run tests.
- `_get_compose_command(self)` (line 412)
- `async _get_project_containers_async(self, project_id)` (line 431): Async version of _get_project_containers.
- `run_docker_ps()` (line 436)
- `_get_project_containers(self, project_id)` (line 477)
- `async execute_command(self, project_id, service, command, timeout, ...)` (line 524): Execute a command inside a service container (Docker) - async version.
- `run_sync_cmd()` (line 548)
- `async exec_in_container(self, project_id, service, command, timeout, ...)` (line 582): Alias for execute_command (API compatibility).
- `async get_preview_url(self, project_id)` (line 592): Get the frontend preview URL for a running sandbox.
- `async get_backend_url(self, project_id)` (line 622): Get the backend API URL for a running sandbox (dynamic port detection).

## app/supervision/quality_gate.py

- `async check_quality_gate(project_id, step_name, quality_score, approved, attempt, ...)` (line 22): Check if workflow should be blocked due to quality issues.  PHASE 4 CHANGE: Only blocks CRITICAL steps with quality < 5.  Returns: (should_block, reas
- `async override_quality_gate(project_id)` (line 73): Allow user to override quality gate.
- `is_blocked(project_id)` (line 79): Check if project is blocked by quality gate.
- `get_block_reason(project_id)` (line 84): Get the reason for quality gate block.

## app/supervision/supervisor.py

- `async marcus_supervise(project_id, manager, agent_name, step_name, agent_output, ...)` (line 22): Marcus reviews an agent's output for quality and correctness.  Returns:     Dict with: approved, quality_score, issues, feedback, corrections
- `async categorize_issue_severity(issue)` (line 530): Categorize Marcus's issues into 'critical' or 'warning' using simple heuristics.
- `async postprocess_marcus_issues(quality, issues)` (line 550): Split issues into critical (must fix) and warnings (nice-to-have).  Returns: (critical_issues, warnings)
- `_extract_archetype(user_request)` (line 570): Extract an archetype identifier from the user request.  Examples: - "Create a bug tracking system" → "bug_tracking" - "Build an admin dashboard" → "ad
- `async supervised_agent_call(project_id, manager, agent_name, step_name, base_instructions, ...)` (line 609): Call an agent with Marcus supervision (ArborMind Muscle).
- `_read_files()` (line 638)

## app/supervision/tiered_review.py

- `get_review_level(file_path)` (line 73): Determine the review level needed for a file.  Args:     file_path: Relative path to the file      Returns:     ReviewLevel indicating how thoroughly 
- `classify_files(files)` (line 105): Classify files by their review level.  Args:     files: List of file dicts with "path" key      Returns:     Dict mapping ReviewLevel to list of files
- `get_review_summary(files)` (line 125): Get a summary of review levels for logging.
- `async tiered_review(files, project_id, manager, agent_name, step_name, ...)` (line 138): Apply tiered review to a list of files.  Returns:     Tuple of (approved_files, review_summary)
- `async parallel_tiered_review(outputs, project_id, manager, user_request)` (line 255): Run tiered reviews in parallel for multiple agent outputs.  This is the key speed optimization - review frontend and backend simultaneously instead of
- `async review_one(output)` (line 271)

## app/tools/executor.py

- `async _parse_and_write_hdap_files(output, branch, step)` (line 44): Extract files from subagentcaller output and write to disk.  CRITICAL: This function is the SOLE OWNER of artifact materialization.  Handles THREE out
- `record_tool_invocation_start(plan_id, invocation, step, agent)` (line 182): Record the start of a tool invocation.  This is where observation happens.
- `record_tool_invocation_end(plan_id, result, step, agent)` (line 212): Record the end of a tool invocation.  PHASE 3: Records tool trace + step exit + failure event (if applicable)
- `async execute_tool_plan(plan, branch, stop_on_failure)` (line 266): Execute a tool plan LINEARLY.  Rules: - No loops - No retries - No self-healing - No reflection  Just execution of an explicit plan.  Args:     plan: 

## app/tools/handler_example.py

- `async handle_backend_models_v2(branch)` (line 51): NEW-STYLE HANDLER: Describes intent, doesn't select tools.  The handler's job is now: 1. Describe the GOAL 2. Call build_tool_plan()  3. Call execute_
- `async handle_architecture_v2(branch)` (line 88): Architecture step - Victoria generates the architecture.md
- `async handle_testing_backend_v2(branch)` (line 102): Testing step - Derek generates tests, then pytest runs them.
- `migrate_handler_to_v2(old_handler_name)` (line 120): Shows how to migrate an old handler to the new style.  OLD:     async def handle_foo(branch):         return await run_tool("subagentcaller", {...})  

## app/tools/implementations.py

- `_get_tit_enabled()` (line 44): Check if TIT is enabled (lazy import to avoid circular deps).
- `_record_tit_event(event)` (line 52): Record TIT event (fire-and-forget).
- `get_sandbox()` (line 64): Get the sandbox singleton (lazy initialization).
- `async _async_run_command(cmd, cwd, timeout, shell)` (line 83): Run a command asynchronously using asyncio.to_thread + subprocess.run. This is Windows-compatible (works with SelectorEventLoop). Returns dict with su
- `run_sync()` (line 95)
- `async tool_sub_agent_caller(args)` (line 239): Call Marcus → Derek/Victoria/Luna sub-agents and normalize their output into a {"files": [...]} shape whenever possible.
- `normalize_files_schema(obj)` (line 311): Normalize arbitrary dict into {'files': [...]}, if possible.
- `async tool_file_writer_batch(args)` (line 471): Write multiple files at once to a base path.
- `async tool_file_reader(args)` (line 496): Read a single file.
- `async tool_file_deleter(args)` (line 520): Delete a file or directory tree.
- `async tool_file_lister(args)` (line 543): List files recursively or non-recursively.
- `async tool_code_viewer(args)` (line 569): Return file contents with some metadata, used for in-UI preview.
- `async tool_bash_runner(args)` (line 597): Run a shell command with timeout (async, non-blocking).
- `async tool_python_executor(args)` (line 612): Execute a snippet of Python code in a temp file (async, non-blocking).
- `async tool_npm_runner(args)` (line 635): Run an npm command, e.g. 'install', 'run build', etc. (async, non-blocking).
- `async tool_pytest_runner(args)` (line 653): Run pytest in a given directory (async, non-blocking).
- `async tool_playwright_runner(args)` (line 673): Run Playwright E2E tests (async, non-blocking).
- `async tool_test_generator(args)` (line 687): Generate simple pytest or Playwright test template.
- `async tool_sandbox_exec(args)` (line 723): Execute a command inside a sandbox container using real Python calls.  - NO HTTP, no FastAPI routes. - Uses the shared SandboxManager singleton. - Aut
- `async validate_deployment(project_path)` (line 922): Validate both frontend and backend deployments.
- `async tool_deployment_validator(args)` (line 962): Tool wrapper for validate_deployment.
- `async tool_key_validator(args)` (line 979): Validate API keys and secrets from environment and .env files.  Features: - Checks both os.environ and .env file - Validates key formats (length, pref
- `async tool_cross_llm_validator(args)` (line 1074): Cross-validate code or output using a secondary LLM provider.  Uses a different LLM than the primary to verify: - Code correctness - Logic consistency
- `async tool_syntax_validator(args)` (line 1183): Validate Python / basic JS/TS syntax.
- `async tool_ux_visualizer(args)` (line 1215): Take a Playwright screenshot of the frontend.
- `async tool_screenshot_comparer(args)` (line 1241): Compare two screenshots pixel-by-pixel.
- `async tool_api_tester(args)` (line 1267): Perform a simple HTTP API call (GET or POST).
- `async tool_web_researcher(args)` (line 1302): Real web research using DuckDuckGo Instant Answer API.  Features: - No API key required - Returns structured results - Falls back to scraping if API r
- `async tool_health_checker(args)` (line 1378): Hit a health endpoint and return status.
- `async tool_user_confirmer(args)` (line 1398): Ask user for confirmation via WebSocket.  In interactive mode: Sends confirmation request to frontend, waits for response In non-interactive mode: Aut
- `async tool_user_prompter(args)` (line 1490): Prompt user for text input via WebSocket.  In interactive mode: Sends input request to frontend, waits for response In non-interactive mode: Returns d
- `async tool_db_schema_reader(args)` (line 1580): Read database schema from the project's models.py file.  Analyzes Beanie Document classes and extracts: - Model names - Field definitions - Field type
- `async tool_db_query_runner(args)` (line 1689): Execute a query against MongoDB using Beanie.  Supports: - find_all: Get all documents from a collection - find_one: Get a single document - count: Co
- `async run_query()` (line 1741)
- `async tool_docker_builder(args)` (line 1778): Build a Docker image using docker CLI (async, non-blocking).
- `async tool_vercel_deployer(args)` (line 1794): Deploy a project using `vercel --prod` (async, non-blocking).
- `async tool_unified_patch_applier(args)` (line 1809)
- `async tool_json_patch_applier(args)` (line 1833)
- `async tool_environment_guard(args)` (line 1865): Comprehensive environment validation with version checking.  Features: - Checks for required CLI tools - Validates version requirements - Checks disk 
- `async tool_static_code_validator(args)` (line 1994): Run static code validation (linting).  P1.1 FIX: This tool was missing from the registry.  NOTE: P1.2 - This is advisory, not authoritative. Failures 
- `async tool_architecture_writer(args)` (line 2057): Generates architecture documentation only.  This is a focused tool that ONLY writes architecture files. It delegates to subagentcaller with Victoria.
- `async tool_router_scaffold_generator(args)` (line 2098): Generates FastAPI router skeletons WITHOUT logic.  This creates the file structure and imports, but does not fill in the endpoint implementations.
- `async tool_router_logic_filler(args)` (line 2183): Fills logic inside an existing router file.  Takes a scaffold and replaces NotImplementedError with actual logic.
- `async tool_code_patch_applier(args)` (line 2234): Applies unified diffs or patches to existing files.  This is a wrapper around the existing patching tools.
- `async run_tool(name, args)` (line 2332)
- `async tool_runtime_bootstrap(args)` (line 2385): PHASE B1: Bootstrap Runtime for Execution Steps  This tool: 1. Detects if backend/frontend exist 2. Attempts to start them (Docker or local) 3. Blocks
- `async _try_docker_runtime(project_path)` (line 2468): Attempt to start runtime via Docker compose.
- `async _try_local_runtime(project_path, has_backend, has_frontend)` (line 2520): Attempt to start runtime locally (uvicorn for backend, npm for frontend).
- `async _check_backend_health(url, timeout)` (line 2562): Check if backend is responding.
- `async _check_frontend_health(url, timeout)` (line 2573): Check if frontend dev server is responding.
- `get_runtime_state()` (line 2583): Get current runtime state (for execution dependency checks).
- `is_runtime_available()` (line 2589): PHASE B2: Check if runtime is available for execution tools.  Used by tools like healthchecker, apitester, playwrightrunner to decide if they should e

## app/tools/migration.py

- `observe_handler(step_name, agent_name)` (line 39): Decorator that adds tool observation to an existing handler.  Usage:     @observe_handler("backend_models", "Derek")     async def step_backend_models
- `decorator(handler_func)` (line 53)
- `async wrapper(branch)` (line 55)
- `_record_handler_start(plan, step, agent)` (line 101): Record handler execution start to ArborMind.
- `_record_handler_end(plan, step, agent, success, duration_ms, ...)` (line 123): Record handler execution end to ArborMind.
- `__init__(self, handler, step_name, agent_name)` (line 186)
- `async __call__(self, branch)` (line 191): Execute handler with observation.
- `wrap_handlers_with_observation(handlers)` (line 228): Wrap all handlers with observation.  Usage:     from app.handlers import HANDLERS     from app.tools.migration import wrap_handlers_with_observation  

## app/tools/patching.py

- `apply_unified_patch(root, patch)` (line 16): Apply a unified diff patch to the workspace. Accepts real git-style unified diffs. Example:     --- a/api.py     +++ b/api.py     @@ -1,4 +1,4 @@
- `_split_into_file_patches(patch)` (line 41): Split a full multi-file unified diff into file-level diffs.
- `_apply_single_file_patch(root, patch_text)` (line 60): Apply unified diff to a single file.
- `_apply_hunks(original, hunk_lines)` (line 112): Apply each @@ hunk to file content.
- `apply_patches(workspace_path, patch_json)` (line 163): Applies LLM patches (JSON-based) across multiple files. Returns a list of results per file.

## app/tools/planner.py

- `__init__(self)` (line 68)
- `async build_tool_plan(self, step, branch, goal, override_args, ...)` (line 71): Build a tool plan for a step.
- `_build_tool_args(self, tool_id, step, branch, goal, ...)` (line 164): Build args for a specific tool.
- `_get_project_path(self, branch)` (line 230): Extract project_path from branch.
- `_get_architecture_file(self, step, project_path)` (line 240): Get the relevant architecture file for a step.
- `_get_context_file(self, step, project_path)` (line 249): Get the relevant code file for context.
- `_should_skip_pre_tool(self, tool_id, step, project_path)` (line 260): Check if a pre-tool should be skipped because its required context doesn't exist.  This prevents unnecessary failures for tools that read files that h
- `_build_subagent_args(self, step, branch, goal)` (line 295): Build args for subagentcaller.
- `get_plan_builder()` (line 320): Get the singleton plan builder.
- `async build_tool_plan(step, branch, goal, override_args)` (line 328): Convenience function to build a tool plan.

## app/tools/planning.py

- `to_dict(self)` (line 37)
- `__post_init__(self)` (line 71)
- `tool_count(self)` (line 77)
- `tool_names(self)` (line 81)
- `to_dict(self)` (line 84)
- `to_dict(self)` (line 109)
- `completed_count(self)` (line 136)
- `failed_count(self)` (line 140)
- `to_dict(self)` (line 143)
- `__init__(self, step, tool_name, error, invocation_id, ...)` (line 160)

## app/tools/registry.py

- `async run_tool(name, args)` (line 11): Run a tool by name.  Uses the consolidated tools.py registry.  Supports both: - run_tool("toolname", args) - run_tool(name="toolname", args=args)
- `get_available_tools()` (line 29): Get list of all available tool names.
- `get_tools_for_step(step)` (line 35): Get tools available for a specific step.
- `async get_relevant_tools_for_query(query, top_k, context_type, archetype, step_name, ...)` (line 41): Find the most relevant tools for a given query and context.  Legacy compatibility function - returns tool definitions.
- `log(module, message)` (line 65): Simple internal logger.

## app/tools/tool_policy.py

- `filter_by_environment(tools)` (line 13): Pre-filter tools by environment constraints.  Prevents tools from being selected if they can't work on current platform.
- `allowed_tools_for_step(step_name, filter_environment)` (line 26): Get allowed tools for a step, optionally filtered by environment.  Args:     step_name: Name of the workflow step     filter_environment: If True, fil

## app/tools/tools.py

- `tool(id, capabilities, phases, description, is_pre, ...)` (line 118): Decorator to register a tool.  Usage:     @tool(         id="subagentcaller",         capabilities=[Capability.GENERATE_CODE],         phases=["*"],  
- `decorator(func)` (line 145)
- `async wrapper(args)` (line 162)
- `async tool_subagentcaller(args)` (line 180): Core LLM caller - delegates to Marcus/sub-agents.
- `async tool_filewriterbatch(args)` (line 198)
- `async tool_filereader(args)` (line 210)
- `async tool_filedeleter(args)` (line 221)
- `async tool_filelister(args)` (line 233)
- `async tool_codeviewer(args)` (line 245)
- `async tool_bashrunner(args)` (line 261)
- `async tool_pythonexecutor(args)` (line 273)
- `async tool_npmrunner(args)` (line 285)
- `async tool_sandboxexec(args)` (line 297)
- `async tool_pytestrunner(args)` (line 313)
- `async tool_playwrightrunner(args)` (line 325)
- `async tool_testgenerator(args)` (line 336)
- `async tool_syntaxvalidator(args)` (line 353)
- `async tool_static_code_validator(args)` (line 366)
- `async tool_deploymentvalidator(args)` (line 378)
- `async tool_keyvalidator(args)` (line 390)
- `async tool_crossllmvalidator(args)` (line 401)
- `async tool_environment_guard(args)` (line 417)
- `async tool_dbschemareader(args)` (line 433)
- `async tool_dbqueryrunner(args)` (line 444)
- `async tool_dockerbuilder(args)` (line 459)
- `async tool_verceldeployer(args)` (line 470)
- `async tool_healthchecker(args)` (line 482)
- `async tool_webresearcher(args)` (line 498)
- `async tool_apitester(args)` (line 510)
- `async tool_uxvisualizer(args)` (line 526)
- `async tool_screenshotcomparer(args)` (line 538)
- `async tool_userconfirmer(args)` (line 553)
- `async tool_userprompter(args)` (line 564)
- `async tool_unifiedpatchapplier(args)` (line 579)
- `async tool_jsonpatchapplier(args)` (line 590)
- `async tool_code_patch_applier(args)` (line 601)
- `async tool_architecture_writer(args)` (line 617)
- `async tool_router_scaffold_generator(args)` (line 628)
- `async tool_router_logic_filler(args)` (line 639)
- `async tool_runtime_bootstrap(args)` (line 656): PHASE B1: Runtime Bootstrap Tool  Runs ONCE after backend_routers, BEFORE any execution step.  Responsibilities: - Detect stack (FastAPI + frontend) -
- `get_tool(tool_id)` (line 684): Get a tool by ID.
- `get_all_tools()` (line 689): Get all registered tools.
- `get_tools_for_phase(phase)` (line 694): Get all tools available for a specific phase.
- `get_tools_with_capability(cap)` (line 702): Get all tools with a specific capability.
- `get_pre_step_tools(phase)` (line 707): Get pre-step tools for a phase.
- `get_post_step_tools(phase)` (line 715): Get post-step tools for a phase.
- `async run_tool(tool_id, args)` (line 723): Run a tool by ID.  Supports both: - run_tool("toolname", args) - run_tool(name="toolname", args=args)
- `_verify_registry()` (line 771): Verify all tools are registered correctly.

## app/tracking/metrics.py

- `update_code_metrics(project_id, agent_name, files)` (line 11): Update code metrics for generated files.
- `get_code_metrics(project_id)` (line 43): Get code metrics for a project.

## app/tracking/quality.py

- `track_quality_score(project_id, agent_name, quality_score, approved)` (line 11): Track quality score for an agent.
- `get_quality_summary(project_id)` (line 27): Get average quality scores per agent.

## app/utils/component_copier.py

- `get_used_components(project_path)` (line 33): Scan frontend source files for @/components/ui/* imports.  Returns:     Set of component names (e.g., {"button", "card", "input"})
- `get_all_required_components(components)` (line 75): Expand component set to include dependencies.  For example, if "alert-dialog" is used, we also need "button".
- `copy_used_components(project_path)` (line 90): Copy only the used Shadcn components to the project.  Returns:     Number of components copied
- `copy_component_by_name(project_path, component_name)` (line 171): Copy a single component by name (for on-demand copying).  Args:     project_path: Path to project     component_name: Component name (e.g., "dialog") 

## app/utils/dependency_fixer.py

- `detect_missing_dependencies(error_output)` (line 11): Parse pytest/pip error output to detect missing dependencies.  Returns a set of package names that need to be installed.
- `_module_to_package(module_name)` (line 56): Convert Python module name to pip package name.  Common mappings for packages where module != package name.
- `add_dependencies_to_requirements(requirements_path, new_deps, version_hints)` (line 91): Add missing dependencies to requirements.txt.  Args:     requirements_path: Path to requirements.txt     new_deps: Set of package names to add     ver
- `auto_fix_backend_dependencies(project_path, error_output)` (line 170): Automatically detect and fix missing backend dependencies.  Args:     project_path: Path to project workspace     error_output: Combined stdout/stderr

## app/utils/entity_classification.py

- `classify_entities_from_mock(project_path)` (line 17): Dynamically classify entities from mock.js structure.  Rules (deterministic, works with ANY entities): 1. AGGREGATE: Appears as top-level export array
- `merge_classifications(mock_classifications, contract_classifications)` (line 102): Merge classifications from multiple sources.  Priority (most reliable to least): 1. mock.js (shows actual data structure)  Default: AGGREGATE (safer t
- `classify_project_entities(project_path)` (line 135): Main entry point: Classify all entities in a project.  This works with ANY user prompt - completely dynamic!  Returns:     Dict mapping entity names t

## app/utils/entity_discovery.py

- `to_dict(self)` (line 35)
- `to_dict(self)` (line 56)
- `to_dict(self)` (line 78)
- `from_dict(cls, data)` (line 90): Create EntitySpec from dictionary.
- `to_dict(self)` (line 111)
- `from_dict(cls, data)` (line 119): Create EntityPlan from dictionary.
- `save(self, path)` (line 129): Save entity plan to JSON file.
- `load(cls, path)` (line 136): Load entity plan from JSON file.
- `clear_discovery_cache(project_path)` (line 146): Clear the entity discovery cache for a project.  Call this after generating new artifacts (contracts.md, models.py, etc.) to ensure fresh discovery re
- `discover_primary_entity(project_path, suppress_warning)` (line 168): Discover the primary entity from project artifacts.  PRIORITY ORDER (authoritative sources only): 1. entity_plan.json (if exists, use first entity) 2.
- `discover_entities_from_architecture(architecture_path)` (line 237): Parse architecture/backend.md or entire architecture directory and extract entities. Returns a list of EntitySpec objects.
- `_extract_from_architecture_legacy(path)` (line 289)
- `_extract_from_models(path)` (line 297): Extract entity from models.py Document class definitions.  IMPORTANT: Only matches ACTUAL class definitions, not commented examples!
- `extract_all_models_from_models_py(project_path)` (line 332): Extract ALL Beanie Document model class names that ACTUALLY EXIST in models.py.  This is used during system_integration to wire only models that Derek
- `extract_document_models_only(project_path)` (line 384): Extract ONLY aggregate Document models (excludes embedded BaseModel classes).  CRITICAL FIX: Prevents wiring embedded models to Beanie, which causes c
- `_extract_from_mock(path)` (line 450)
- `_extract_from_routers(routers_dir)` (line 465)
- `_extract_from_user_request(path)` (line 478)
- `singularize(word)` (line 498): Convert a plural word to singular form.  This is the SINGLE SOURCE OF TRUTH for singularization. Import this function instead of creating inline copie
- `extract_entity_from_request(user_request)` (line 544): Dynamically extract a potential entity name from the user request.  This is the SINGLE SOURCE OF TRUTH for entity extraction from user requests. Impor
- `discover_db_function(project_path)` (line 611): Discover the database initialization function name from database.py.  Scans the actual database.py file to find the correct function name. Falls back 
- `discover_routers(project_path)` (line 653)
- `get_entity_plural(entity_name)` (line 663): Get the plural form of an entity name.  This is a wrapper around the centralized pluralize function.
- `discover_all_entities(project_path, user_request)` (line 677): Discover ALL entities from project artifacts or user request.  Returns at least 1 entity, up to MAX_ENTITIES.  Priority: 1. Extract from architecture.
- `_extract_all_entities_from_architecture(architecture_path)` (line 773): Parse architecture.md for ALL entities in 'Data Models' section.
- `detect_relationships(project_path, entities)` (line 830): Detect relationships from architecture.md.  Args:     project_path: Project directory     entities: List of discovered entities  Returns:     List of 

## app/utils/integration_playbooks.py

- `apply_playbook(playbook_name, project_path)` (line 46): Apply integration playbook to project

## app/utils/parser.py

- `parse_hdap(raw_output)` (line 53): Parse HDAP-formatted LLM output into files dictionary.  STRICT MODE: - If HDAP markers present → parse only those - If HDAP markers absent → return em
- `_is_valid_file_path(path)` (line 127): Check if a path looks like a valid file path.
- `normalize_llm_output(raw_output, step_name)` (line 148): Parse LLM output into standardized format.  HDAP-only parsing - STRICT protocol enforcement.  Args:     raw_output: Raw LLM response     step_name: St
- `is_output_complete(parsed)` (line 189): Check if parsed output is complete (no truncation).  Returns True if: - All files have matching END_FILE markers - At least one file was extracted - H
- `get_incomplete_files(parsed)` (line 203): Get list of files that were truncated (missing END_FILE).
- `has_hdap_markers(parsed)` (line 208): Check if the output contained HDAP markers at all.
- `parse_json_metadata(raw)` (line 217): Parse JSON metadata from LLM output.  Use this for steps that return STRUCTURED DATA (not files): - Analysis step (domain, entities, features) - Revie
- `parse_json(raw)` (line 270): Legacy JSON parser - delegates to parse_json_metadata.  Kept for backwards compatibility.
- `sanitize_marcus_output(raw)` (line 279): Legacy function - now just returns cleaned input.
- `normalize_unicode_aggressively(content, filepath)` (line 290): Aggressively normalize Unicode to ASCII in code content.  LLMs sometimes output fancy Unicode characters that break parsers: - Smart quotes ("" '' ins

## app/utils/path_utils.py

- `get_project_path(project_id)` (line 19): Get the absolute path to a project workspace.  This is the SINGLE SOURCE OF TRUTH for project path resolution. Import this function instead of constru
- `get_backend_path(project_id)` (line 39): Get the backend directory path for a project.
- `get_frontend_path(project_id)` (line 44): Get the frontend directory path for a project.
- `get_backend_app_path(project_id)` (line 49): Get the backend app directory path for a project.
- `get_routers_path(project_id)` (line 54): Get the routers directory path for a project.
- `get_models_path(project_id)` (line 59): Get the models.py file path for a project.
- `get_main_py_path(project_id)` (line 64): Get the main.py file path for a project.
- `get_architecture_path(project_id)` (line 69): Get the architecture.md file path for a project.
- `get_tests_path(project_id)` (line 74): Get the backend tests directory path for a project.
- `is_valid_project_path(path)` (line 79): Check if a path is within the workspaces directory.  Security check to prevent path traversal attacks.
- `ensure_project_directories(project_id)` (line 93): Ensure all required project directories exist.  Creates: - backend/app/routers - backend/tests - frontend/src/pages - frontend/src/components - fronte

## app/utils/test_scaffolding.py

- `create_robust_smoke_test()` (line 10): Create a robust smoke test using the standard testing contract testids (ESM friendly).
- `extract_testids_from_project(project_path)` (line 58): Extract all data-testid values from JSX/TSX files in the project. Returns a list of testid strings that actually exist in the UI.
- `create_matching_smoke_test(project_path)` (line 90): Create a smoke test that checks for elements that ACTUALLY exist in the UI. This handles mutually exclusive UI states (loading, error, content).
- `get_available_selectors(project_path)` (line 183): Analyze the project to find all available selectors for testing. Returns a dict with categories of selectors.

## app/utils/ui_beautifier.py

- `beautify_frontend_files(files)` (line 12): Post-process frontend files to ensure UI consistency.  Applies: - Standard page shell classes (min-h-screen, max-w-6xl, etc.) - Normalized spacing cla
- `_beautify_jsx(content, filepath)` (line 68): Apply beautification rules to JSX content.
- `_normalize_spacing(content)` (line 81): Normalize inconsistent spacing classes for consistency.
- `_ensure_page_testids(content)` (line 102): Ensure required testids are present in page components.
- `ensure_page_shell(content)` (line 145): Ensure page has standard shell wrapper classes.  Expected pattern: <main data-testid="page-root" className="min-h-screen bg-background">   <div classN

## app/validation/static_validator.py

- `__init__(self, step_name)` (line 30)
- `to_dict(self)` (line 40): Convert evidence to dictionary for aggregator.
- `__init__(self, project_path)` (line 73)
- `validate_backend_step(self, step_name)` (line 76): Collect evidence about backend implementation.  Observes: - Python syntax validity - FastAPI routes defined - MongoDB models present - Import statemen
- `validate_frontend_step(self, step_name)` (line 143): Collect evidence about frontend implementation.  Observes: - JavaScript/JSX syntax validity - Component files present - API client exists - Routes def
- `_analyze_python_file(self, file_path)` (line 185): Analyze a Python file and collect evidence.  Returns:     Dictionary with syntax_errors, routes, models found
- `validate_testing_step(self, step_name, test_type)` (line 236): Collect evidence about test files (without running them).  Observes: - Test files exist - Test syntax is valid - Test structure looks correct  Args:  

## app/validation/syntax_validator.py

- `__init__(self, valid, errors, warnings, fixed_content, ...)` (line 24)
- `__bool__(self)` (line 36)
- `to_dict(self)` (line 39)
- `assert_no_empty_defs(path, content)` (line 52): Check for empty function/class definitions (only pass/docstring).
- `check_undefined_names(code, filename)` (line 78): Check for undefined names in Python code (missing imports).  Uses AST to: 1. Collect all defined names (imports, classes, functions, variables) 2. Fin
- `extract_names_from_annotation(annotation_node)` (line 160): Recursively extract all names from a type annotation.
- `validate_python_syntax(code, filename)` (line 212): Validate Python code using AST parsing.  Catches and AUTO-FIXES: - Malformed imports (multiple imports on one line)  Catches: - Syntax errors - Incomp
- `fix_index_tuple(match)` (line 299)
- `validate_javascript_syntax(code, filename)` (line 353): Basic JavaScript/JSX validation without a full parser.  AUTO-FIXES: - Errant backslashes after comments (common LLM mistake)
- `check_duplicate_attributes(code, filename)` (line 459): Check for duplicate HTML/JSX attributes on the same element.  Common LLM mistake:     <main data-testid="page-root" className="..." data-testid="home-
- `validate_syntax(path, content)` (line 503): Validate file syntax based on its extension.  GATE 1: Unicode Normalization (strips ALL non-ASCII) GATE 2: Syntax validation (AST / parser)  This is t
- `validate_files_batch(files)` (line 557): Validate a batch of files from LLM output.  Args:     files: List of {"path": str, "content": str} dicts  Returns:     Tuple of (valid_files, invalid_
- `preflight_check(agent_output)` (line 603): Pre-flight validation gate for agent output.  This is the main entry point, called BEFORE Marcus review.  Args:     agent_output: The parsed agent res

## app/workflow/__init__.py

- `__getattr__(name)` (line 8): Lazy import to avoid circular dependencies.

## app/workflow/engine.py

- `async run_workflow(project_id, description, workspaces_path, manager, provider, ...)` (line 19): Start a new workflow for a project using FASTOrchestratorV2.
- `agent_artifact_filter(src, names)` (line 62)
- `async autonomous_agent_workflow(project_id, description, workspaces_path, manager, provider, ...)` (line 179): Backwards-compatible wrapper.
- `async resume_workflow(project_id, user_message, manager, workspaces_dir)` (line 191): Resume a paused workflow OR start a refine workflow. Uses FASTOrchestratorV2 for execution.
- `async resume_from_checkpoint_workflow(project_id, description, workspaces_path, manager, provider, ...)` (line 251): Resume a workflow explicitly from saved checkpoint (UI triggered).

## app/workflow/integration_example.py

- `example_workflow_completion()` (line 17): Example: How to use outcome aggregation at end of workflow.
- `example_hard_failure_precedence()` (line 85): Example: HARD_FAILURE in isolated step still fails workflow.  This demonstrates ADJUSTMENT 1.
